{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVxWZGJxprU"
   },
   "source": [
    "# Question B2 (10 marks)\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EycCozG06Duu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-widedeep in d:\\program files\\python310\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.21.6 in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (1.4.1.post1)\n",
      "Requirement already satisfied: einops in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (0.7.0)\n",
      "Requirement already satisfied: pandas>=1.3.5 in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (2.0.1)\n",
      "Requirement already satisfied: spacy in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (3.7.4)\n",
      "Requirement already satisfied: imutils in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (0.5.4)\n",
      "Requirement already satisfied: scipy>=1.7.3 in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (1.10.1)\n",
      "Requirement already satisfied: gensim in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (4.3.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (2.1.2+cu121)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (0.16.2+cu121)\n",
      "Requirement already satisfied: wrapt in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (1.15.0)\n",
      "Requirement already satisfied: torchmetrics in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (1.2.1)\n",
      "Requirement already satisfied: pyarrow in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (13.0.0)\n",
      "Requirement already satisfied: opencv-contrib-python in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (4.9.0.80)\n",
      "Requirement already satisfied: fastparquet>=0.8.1 in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (2024.2.0)\n",
      "Requirement already satisfied: tqdm in d:\\program files\\python310\\lib\\site-packages (from pytorch-widedeep) (4.65.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in d:\\program files\\python310\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2.8.2)\n",
      "Requirement already satisfied: packaging in d:\\program files\\python310\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (23.1)\n",
      "Requirement already satisfied: fsspec in d:\\program files\\python310\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2023.6.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\program files\\python310\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\program files\\python310\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\program files\\python310\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\program files\\python310\\lib\\site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\program files\\python310\\lib\\site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (1.2.0)\n",
      "Requirement already satisfied: sympy in d:\\program files\\python310\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (1.11.1)\n",
      "Requirement already satisfied: networkx in d:\\program files\\python310\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (3.0)\n",
      "Requirement already satisfied: filelock in d:\\program files\\python310\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (3.9.0)\n",
      "Requirement already satisfied: jinja2 in d:\\program files\\python310\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in d:\\program files\\python310\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (4.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\program files\\python310\\lib\\site-packages (from torchvision>=0.15.0->pytorch-widedeep) (9.5.0)\n",
      "Requirement already satisfied: requests in d:\\program files\\python310\\lib\\site-packages (from torchvision>=0.15.0->pytorch-widedeep) (2.28.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\program files\\python310\\lib\\site-packages (from gensim->pytorch-widedeep) (6.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\program files\\python310\\lib\\site-packages (from spacy->pytorch-widedeep) (3.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\program files\\python310\\lib\\site-packages (from spacy->pytorch-widedeep) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\program files\\python310\\lib\\site-packages (from spacy->pytorch-widedeep) (1.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\program files\\python310\\lib\\site-packages (from spacy->pytorch-widedeep) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in d:\\program files\\python310\\lib\\site-packages (from spacy->pytorch-widedeep) (0.3.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\program files\\python310\\lib\\site-packages (from spacy->pytorch-widedeep) (1.10.14)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\program files\\python310\\lib\\site-packages (from spacy->pytorch-widedeep) (3.0.12)\n",
      "Requirement already satisfied: setuptools in d:\\program files\\python310\\lib\\site-packages (from spacy->pytorch-widedeep) (69.0.3)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in d:\\program files\\python310\\lib\\site-packages (from spacy->pytorch-widedeep) (8.2.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\program files\\python310\\lib\\site-packages (from spacy->pytorch-widedeep) (1.0.5)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in d:\\program files\\python310\\lib\\site-packages (from spacy->pytorch-widedeep) (0.9.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\program files\\python310\\lib\\site-packages (from spacy->pytorch-widedeep) (2.4.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\program files\\python310\\lib\\site-packages (from spacy->pytorch-widedeep) (1.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\program files\\python310\\lib\\site-packages (from spacy->pytorch-widedeep) (3.3.0)\n",
      "Requirement already satisfied: colorama in d:\\program files\\python310\\lib\\site-packages (from tqdm->pytorch-widedeep) (0.4.6)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in d:\\program files\\python310\\lib\\site-packages (from torchmetrics->pytorch-widedeep) (0.10.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\program files\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-widedeep) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\program files\\python310\\lib\\site-packages (from requests->torchvision>=0.15.0->pytorch-widedeep) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\program files\\python310\\lib\\site-packages (from requests->torchvision>=0.15.0->pytorch-widedeep) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\program files\\python310\\lib\\site-packages (from requests->torchvision>=0.15.0->pytorch-widedeep) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\program files\\python310\\lib\\site-packages (from requests->torchvision>=0.15.0->pytorch-widedeep) (1.26.13)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\program files\\python310\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy->pytorch-widedeep) (0.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in d:\\program files\\python310\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy->pytorch-widedeep) (0.7.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\program files\\python310\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy->pytorch-widedeep) (8.1.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in d:\\program files\\python310\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy->pytorch-widedeep) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\program files\\python310\\lib\\site-packages (from jinja2->torch>=2.0.0->pytorch-widedeep) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\program files\\python310\\lib\\site-packages (from sympy->torch>=2.0.0->pytorch-widedeep) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lq0elU0J53Yo"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU3xdVpwzuLx"
   },
   "source": [
    "1.Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_oYG6lNIh7Mp"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "train_data = df[df['year'] <= 2020]\n",
    "test_data = df[df['year'] >= 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_q9PoR50JAA"
   },
   "source": [
    "2.Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZBY1iqUXtYWn",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 1366/1366 [00:11<00:00, 122.42it/s, loss=2.39e+5, metrics={'r2': -2.4291}]\n",
      "epoch 2: 100%|██████████| 1366/1366 [00:09<00:00, 141.81it/s, loss=8.22e+4, metrics={'r2': 0.6577}]\n",
      "epoch 3: 100%|██████████| 1366/1366 [00:09<00:00, 141.82it/s, loss=6.31e+4, metrics={'r2': 0.81}]  \n",
      "epoch 4: 100%|██████████| 1366/1366 [00:09<00:00, 149.18it/s, loss=5.81e+4, metrics={'r2': 0.8436}]\n",
      "epoch 5: 100%|██████████| 1366/1366 [00:09<00:00, 144.47it/s, loss=5.57e+4, metrics={'r2': 0.8588}]\n",
      "epoch 6: 100%|██████████| 1366/1366 [00:10<00:00, 133.30it/s, loss=5.44e+4, metrics={'r2': 0.8664}]\n",
      "epoch 7: 100%|██████████| 1366/1366 [00:09<00:00, 137.68it/s, loss=5.36e+4, metrics={'r2': 0.8708}]\n",
      "epoch 8: 100%|██████████| 1366/1366 [00:09<00:00, 143.37it/s, loss=5.32e+4, metrics={'r2': 0.8733}]\n",
      "epoch 9: 100%|██████████| 1366/1366 [00:09<00:00, 142.80it/s, loss=5.3e+4, metrics={'r2': 0.8737}] \n",
      "epoch 10: 100%|██████████| 1366/1366 [00:09<00:00, 148.05it/s, loss=5.27e+4, metrics={'r2': 0.8755}]\n",
      "epoch 11: 100%|██████████| 1366/1366 [00:10<00:00, 131.31it/s, loss=5.25e+4, metrics={'r2': 0.8766}]\n",
      "epoch 12: 100%|██████████| 1366/1366 [00:10<00:00, 130.35it/s, loss=5.23e+4, metrics={'r2': 0.8777}]\n",
      "epoch 13: 100%|██████████| 1366/1366 [00:10<00:00, 126.91it/s, loss=5.2e+4, metrics={'r2': 0.8794}] \n",
      "epoch 14: 100%|██████████| 1366/1366 [00:11<00:00, 123.48it/s, loss=5.16e+4, metrics={'r2': 0.8809}]\n",
      "epoch 15: 100%|██████████| 1366/1366 [00:11<00:00, 122.20it/s, loss=5.13e+4, metrics={'r2': 0.8826}]\n",
      "epoch 16: 100%|██████████| 1366/1366 [00:10<00:00, 125.59it/s, loss=5.05e+4, metrics={'r2': 0.8863}]\n",
      "epoch 17: 100%|██████████| 1366/1366 [00:09<00:00, 137.27it/s, loss=5.01e+4, metrics={'r2': 0.8881}]\n",
      "epoch 18: 100%|██████████| 1366/1366 [00:10<00:00, 135.45it/s, loss=4.91e+4, metrics={'r2': 0.8925}]\n",
      "epoch 19: 100%|██████████| 1366/1366 [00:10<00:00, 129.13it/s, loss=4.83e+4, metrics={'r2': 0.896}] \n",
      "epoch 20: 100%|██████████| 1366/1366 [00:10<00:00, 132.18it/s, loss=4.76e+4, metrics={'r2': 0.8988}]\n",
      "epoch 21: 100%|██████████| 1366/1366 [00:10<00:00, 133.04it/s, loss=4.67e+4, metrics={'r2': 0.9024}]\n",
      "epoch 22: 100%|██████████| 1366/1366 [00:10<00:00, 133.21it/s, loss=4.62e+4, metrics={'r2': 0.9047}]\n",
      "epoch 23: 100%|██████████| 1366/1366 [00:09<00:00, 147.39it/s, loss=4.56e+4, metrics={'r2': 0.9068}]\n",
      "epoch 24: 100%|██████████| 1366/1366 [00:08<00:00, 154.48it/s, loss=4.53e+4, metrics={'r2': 0.9082}]\n",
      "epoch 25: 100%|██████████| 1366/1366 [00:08<00:00, 154.76it/s, loss=4.52e+4, metrics={'r2': 0.9083}]\n",
      "epoch 26: 100%|██████████| 1366/1366 [00:10<00:00, 127.67it/s, loss=4.47e+4, metrics={'r2': 0.9102}]\n",
      "epoch 27: 100%|██████████| 1366/1366 [00:10<00:00, 133.33it/s, loss=4.47e+4, metrics={'r2': 0.9105}]\n",
      "epoch 28: 100%|██████████| 1366/1366 [00:09<00:00, 139.98it/s, loss=4.44e+4, metrics={'r2': 0.9117}]\n",
      "epoch 29: 100%|██████████| 1366/1366 [00:09<00:00, 140.71it/s, loss=4.43e+4, metrics={'r2': 0.9116}]\n",
      "epoch 30: 100%|██████████| 1366/1366 [00:09<00:00, 138.83it/s, loss=4.42e+4, metrics={'r2': 0.9123}]\n",
      "epoch 31: 100%|██████████| 1366/1366 [00:09<00:00, 136.61it/s, loss=4.41e+4, metrics={'r2': 0.9124}]\n",
      "epoch 32: 100%|██████████| 1366/1366 [00:09<00:00, 143.75it/s, loss=4.42e+4, metrics={'r2': 0.912}] \n",
      "epoch 33: 100%|██████████| 1366/1366 [00:09<00:00, 143.75it/s, loss=4.41e+4, metrics={'r2': 0.9126}]\n",
      "epoch 34: 100%|██████████| 1366/1366 [00:10<00:00, 130.87it/s, loss=4.4e+4, metrics={'r2': 0.9127}] \n",
      "epoch 35: 100%|██████████| 1366/1366 [00:10<00:00, 136.04it/s, loss=4.4e+4, metrics={'r2': 0.9127}] \n",
      "epoch 36: 100%|██████████| 1366/1366 [00:10<00:00, 125.19it/s, loss=4.38e+4, metrics={'r2': 0.9138}]\n",
      "epoch 37: 100%|██████████| 1366/1366 [00:10<00:00, 131.10it/s, loss=4.39e+4, metrics={'r2': 0.9131}]\n",
      "epoch 38: 100%|██████████| 1366/1366 [00:10<00:00, 130.32it/s, loss=4.37e+4, metrics={'r2': 0.9139}]\n",
      "epoch 39: 100%|██████████| 1366/1366 [00:10<00:00, 132.17it/s, loss=4.36e+4, metrics={'r2': 0.9142}]\n",
      "epoch 40: 100%|██████████| 1366/1366 [00:10<00:00, 131.17it/s, loss=4.37e+4, metrics={'r2': 0.9139}]\n",
      "epoch 41: 100%|██████████| 1366/1366 [00:11<00:00, 123.91it/s, loss=4.37e+4, metrics={'r2': 0.9139}]\n",
      "epoch 42: 100%|██████████| 1366/1366 [00:11<00:00, 121.18it/s, loss=4.34e+4, metrics={'r2': 0.9151}]\n",
      "epoch 43: 100%|██████████| 1366/1366 [00:12<00:00, 112.05it/s, loss=4.35e+4, metrics={'r2': 0.9145}]\n",
      "epoch 44: 100%|██████████| 1366/1366 [00:11<00:00, 114.92it/s, loss=4.33e+4, metrics={'r2': 0.9152}]\n",
      "epoch 45: 100%|██████████| 1366/1366 [00:11<00:00, 116.22it/s, loss=4.35e+4, metrics={'r2': 0.9146}]\n",
      "epoch 46: 100%|██████████| 1366/1366 [00:11<00:00, 116.03it/s, loss=4.36e+4, metrics={'r2': 0.914}] \n",
      "epoch 47: 100%|██████████| 1366/1366 [00:11<00:00, 120.98it/s, loss=4.33e+4, metrics={'r2': 0.9154}]\n",
      "epoch 48: 100%|██████████| 1366/1366 [00:11<00:00, 120.87it/s, loss=4.32e+4, metrics={'r2': 0.9154}]\n",
      "epoch 49: 100%|██████████| 1366/1366 [00:11<00:00, 120.72it/s, loss=4.32e+4, metrics={'r2': 0.9156}]\n",
      "epoch 50: 100%|██████████| 1366/1366 [00:11<00:00, 115.47it/s, loss=4.33e+4, metrics={'r2': 0.9154}]\n",
      "epoch 51: 100%|██████████| 1366/1366 [00:12<00:00, 112.56it/s, loss=4.32e+4, metrics={'r2': 0.9159}]\n",
      "epoch 52: 100%|██████████| 1366/1366 [00:11<00:00, 118.81it/s, loss=4.29e+4, metrics={'r2': 0.9168}]\n",
      "epoch 53: 100%|██████████| 1366/1366 [00:11<00:00, 118.66it/s, loss=4.31e+4, metrics={'r2': 0.9161}]\n",
      "epoch 54: 100%|██████████| 1366/1366 [00:11<00:00, 120.42it/s, loss=4.31e+4, metrics={'r2': 0.9159}]\n",
      "epoch 55: 100%|██████████| 1366/1366 [00:11<00:00, 120.35it/s, loss=4.33e+4, metrics={'r2': 0.9157}]\n",
      "epoch 56: 100%|██████████| 1366/1366 [00:11<00:00, 120.26it/s, loss=4.32e+4, metrics={'r2': 0.9157}]\n",
      "epoch 57: 100%|██████████| 1366/1366 [00:11<00:00, 119.71it/s, loss=4.3e+4, metrics={'r2': 0.9162}] \n",
      "epoch 58: 100%|██████████| 1366/1366 [00:11<00:00, 119.78it/s, loss=4.3e+4, metrics={'r2': 0.9164}] \n",
      "epoch 59: 100%|██████████| 1366/1366 [00:11<00:00, 119.68it/s, loss=4.3e+4, metrics={'r2': 0.9164}] \n",
      "epoch 60: 100%|██████████| 1366/1366 [00:11<00:00, 121.19it/s, loss=4.29e+4, metrics={'r2': 0.9166}]\n",
      "epoch 61: 100%|██████████| 1366/1366 [00:11<00:00, 120.02it/s, loss=4.29e+4, metrics={'r2': 0.9166}]\n",
      "epoch 62: 100%|██████████| 1366/1366 [00:11<00:00, 118.67it/s, loss=4.29e+4, metrics={'r2': 0.9165}]\n",
      "epoch 63: 100%|██████████| 1366/1366 [00:12<00:00, 108.02it/s, loss=4.29e+4, metrics={'r2': 0.9167}]\n",
      "epoch 64: 100%|██████████| 1366/1366 [00:11<00:00, 120.39it/s, loss=4.27e+4, metrics={'r2': 0.9174}]\n",
      "epoch 65: 100%|██████████| 1366/1366 [00:11<00:00, 116.80it/s, loss=4.29e+4, metrics={'r2': 0.9168}]\n",
      "epoch 66: 100%|██████████| 1366/1366 [00:09<00:00, 144.94it/s, loss=4.29e+4, metrics={'r2': 0.9169}]\n",
      "epoch 67: 100%|██████████| 1366/1366 [00:09<00:00, 147.64it/s, loss=4.27e+4, metrics={'r2': 0.9172}]\n",
      "epoch 68: 100%|██████████| 1366/1366 [00:10<00:00, 125.13it/s, loss=4.27e+4, metrics={'r2': 0.9171}]\n",
      "epoch 69: 100%|██████████| 1366/1366 [00:11<00:00, 120.85it/s, loss=4.26e+4, metrics={'r2': 0.9178}]\n",
      "epoch 70: 100%|██████████| 1366/1366 [00:11<00:00, 121.14it/s, loss=4.27e+4, metrics={'r2': 0.9176}]\n",
      "epoch 71: 100%|██████████| 1366/1366 [00:10<00:00, 131.88it/s, loss=4.25e+4, metrics={'r2': 0.9183}]\n",
      "epoch 72: 100%|██████████| 1366/1366 [00:10<00:00, 131.42it/s, loss=4.25e+4, metrics={'r2': 0.9183}]\n",
      "epoch 73: 100%|██████████| 1366/1366 [00:09<00:00, 142.95it/s, loss=4.26e+4, metrics={'r2': 0.9177}]\n",
      "epoch 74: 100%|██████████| 1366/1366 [00:09<00:00, 142.13it/s, loss=4.23e+4, metrics={'r2': 0.9191}]\n",
      "epoch 75: 100%|██████████| 1366/1366 [00:10<00:00, 133.77it/s, loss=4.24e+4, metrics={'r2': 0.9185}]\n",
      "epoch 76: 100%|██████████| 1366/1366 [00:10<00:00, 136.21it/s, loss=4.25e+4, metrics={'r2': 0.9183}]\n",
      "epoch 77: 100%|██████████| 1366/1366 [00:09<00:00, 141.64it/s, loss=4.24e+4, metrics={'r2': 0.9185}]\n",
      "epoch 78: 100%|██████████| 1366/1366 [00:09<00:00, 137.62it/s, loss=4.24e+4, metrics={'r2': 0.9185}]\n",
      "epoch 79: 100%|██████████| 1366/1366 [00:09<00:00, 137.63it/s, loss=4.24e+4, metrics={'r2': 0.9186}]\n",
      "epoch 80: 100%|██████████| 1366/1366 [00:09<00:00, 138.09it/s, loss=4.21e+4, metrics={'r2': 0.9196}]\n",
      "epoch 81: 100%|██████████| 1366/1366 [00:10<00:00, 135.04it/s, loss=4.24e+4, metrics={'r2': 0.9185}]\n",
      "epoch 82: 100%|██████████| 1366/1366 [00:10<00:00, 136.28it/s, loss=4.23e+4, metrics={'r2': 0.9192}]\n",
      "epoch 83: 100%|██████████| 1366/1366 [00:10<00:00, 135.66it/s, loss=4.22e+4, metrics={'r2': 0.9195}]\n",
      "epoch 84: 100%|██████████| 1366/1366 [00:09<00:00, 137.83it/s, loss=4.22e+4, metrics={'r2': 0.9194}]\n",
      "epoch 85: 100%|██████████| 1366/1366 [00:09<00:00, 137.07it/s, loss=4.2e+4, metrics={'r2': 0.92}]   \n",
      "epoch 86: 100%|██████████| 1366/1366 [00:10<00:00, 133.43it/s, loss=4.21e+4, metrics={'r2': 0.9197}]\n",
      "epoch 87: 100%|██████████| 1366/1366 [00:10<00:00, 125.86it/s, loss=4.2e+4, metrics={'r2': 0.92}]   \n",
      "epoch 88: 100%|██████████| 1366/1366 [00:10<00:00, 128.57it/s, loss=4.2e+4, metrics={'r2': 0.9203}] \n",
      "epoch 89: 100%|██████████| 1366/1366 [00:10<00:00, 132.20it/s, loss=4.19e+4, metrics={'r2': 0.9206}]\n",
      "epoch 90: 100%|██████████| 1366/1366 [00:10<00:00, 130.30it/s, loss=4.19e+4, metrics={'r2': 0.9206}]\n",
      "epoch 91: 100%|██████████| 1366/1366 [00:10<00:00, 129.71it/s, loss=4.19e+4, metrics={'r2': 0.9206}]\n",
      "epoch 92: 100%|██████████| 1366/1366 [00:10<00:00, 124.76it/s, loss=4.16e+4, metrics={'r2': 0.9214}]\n",
      "epoch 93: 100%|██████████| 1366/1366 [00:10<00:00, 134.43it/s, loss=4.15e+4, metrics={'r2': 0.922}] \n",
      "epoch 94: 100%|██████████| 1366/1366 [00:10<00:00, 134.11it/s, loss=4.17e+4, metrics={'r2': 0.9214}]\n",
      "epoch 95: 100%|██████████| 1366/1366 [00:09<00:00, 141.10it/s, loss=4.16e+4, metrics={'r2': 0.922}] \n",
      "epoch 96: 100%|██████████| 1366/1366 [00:10<00:00, 136.52it/s, loss=4.16e+4, metrics={'r2': 0.9218}]\n",
      "epoch 97: 100%|██████████| 1366/1366 [00:10<00:00, 136.01it/s, loss=4.16e+4, metrics={'r2': 0.9218}]\n",
      "epoch 98: 100%|██████████| 1366/1366 [00:10<00:00, 128.25it/s, loss=4.15e+4, metrics={'r2': 0.9222}]\n",
      "epoch 99: 100%|██████████| 1366/1366 [00:11<00:00, 116.71it/s, loss=4.16e+4, metrics={'r2': 0.9218}]\n",
      "epoch 100: 100%|██████████| 1366/1366 [00:09<00:00, 143.30it/s, loss=4.14e+4, metrics={'r2': 0.9226}]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE & RESULT HERE\n",
    "# categorical columns\n",
    "cat_embed_cols = ['month', 'town', 'flat_model_type', 'storey_range']\n",
    "\n",
    "# continuous columns\n",
    "continuous_cols = ['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm']\n",
    "\n",
    "# create deeptabular\n",
    "tab_preprocessor = TabPreprocessor(\n",
    "    cat_embed_cols=cat_embed_cols,\n",
    "    continuous_cols=continuous_cols,\n",
    "    cols_to_scale=continuous_cols\n",
    ")\n",
    "\n",
    "# preprocess data\n",
    "train_target = train_data['resale_price'].values\n",
    "test_target = test_data['resale_price'].values\n",
    "train_data = tab_preprocessor.fit_transform(train_data)\n",
    "test_data = tab_preprocessor.transform(test_data)\n",
    "\n",
    "# initialize model\n",
    "tabmlp = TabMlp(\n",
    "    mlp_hidden_dims=[200, 100],\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "    continuous_cols=continuous_cols,\n",
    ")\n",
    "model = WideDeep(deeptabular=tabmlp)  # since trainer needs WideDeep class\n",
    "\n",
    "# create the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    objective='rmse',\n",
    "    metrics=[R2Score],\n",
    "    num_workers=0,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# training\n",
    "trainer.fit(\n",
    "    X_tab = train_data,\n",
    "    target = train_target,\n",
    "    n_epochs=100,\n",
    "    batch_size=64\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V46s-MdM0y5c"
   },
   "source": [
    "3.Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KAhAgvMC07g6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 1128/1128 [00:03<00:00, 334.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 99218.29356196288\n",
      "R2: 0.6560605147559431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE & RESULT HERE\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# test the model\n",
    "res = trainer.predict(X_tab=test_data, batch_size=64)\n",
    "\n",
    "# report RMSE and R2\n",
    "RMSE = np.sqrt(mean_squared_error(test_target, res))\n",
    "r2 = r2_score(test_target, res)\n",
    "\n",
    "print(f\"RMSE: {RMSE}\")\n",
    "print(f\"R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
