{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'glove-twitter-25'\n",
    "wv_path = ['./data/train_data/train', './data/test_data/test']\n",
    "\n",
    "with open(wv_path[0] + f'_{model_type}.pkl', 'rb') as f:\n",
    "    X_train, y_train = pickle.load(f)\n",
    "with open(wv_path[1] + f'_{model_type}.pkl', 'rb') as f:\n",
    "    X_test, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train[:200], y_train[:200]\n",
    "X_test, y_test = X_test[:50], y_test[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data [200, 25, 54]\n",
      "Shape of training data [50, 25, 54]\n"
     ]
    }
   ],
   "source": [
    "# data process\n",
    "max_length = max(len(i) for i in X_test + X_train)\n",
    "\n",
    "def preprocess_dataset(X):\n",
    "    X_list = []\n",
    "    for i in X:\n",
    "        temp = i\n",
    "        for _ in range(max_length - len(i)):\n",
    "            temp.append(np.zeros(25))\n",
    "        np_array = np.array(temp)\n",
    "        trans = np.transpose(np_array)\n",
    "        X_list.append(torch.from_numpy(trans))\n",
    "    X_tensor = torch.stack(X_list)\n",
    "    return X_tensor\n",
    "\n",
    "X_train_tensor = preprocess_dataset(X_train)\n",
    "print(f\"Shape of training data {[X_train_tensor.shape[i] for i in range(3)]}\")\n",
    "X_test_tensor = preprocess_dataset(X_test)\n",
    "print(f\"Shape of training data {[X_test_tensor.shape[i] for i in range(3)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of label types: 12\n"
     ]
    }
   ],
   "source": [
    "# process labels\n",
    "labels = list(set(y_train + y_test))\n",
    "label_num = len(labels)\n",
    "for i in range(len(y_train)):\n",
    "    y_train[i] = labels.index(y_train[i])\n",
    "for i in range(len(y_test)):\n",
    "    y_test[i] = labels.index(y_test[i])\n",
    "print(f\"Number of label types: {label_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chihao Shen\\AppData\\Local\\Temp\\ipykernel_9348\\166145593.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X =torch.tensor(X, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X =torch.tensor(X, dtype=torch.float)\n",
    "        self.y =torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def initialize_loader(X_train_tensor, X_test_tensor, y_train, y_test):\n",
    "    # no need to do the scale since original wv already did\n",
    "    train_data = CustomDataset(X_train_tensor, y_train)\n",
    "    test_data = CustomDataset(X_test_tensor, y_test)\n",
    "\n",
    "    # convert to DataLoader for batch processing and shuffling\n",
    "    train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=50, shuffle=False)\n",
    "    for inputs, targets in test_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = initialize_loader(X_train_tensor, X_test_tensor, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, dim_in, dim_conv, dim_out, dropout_rate=0.5, l2_norm=3):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1_3 = nn.Conv1d(dim_in, dim_conv, 3, padding=5)  # 33\n",
    "        self.conv1_4 = nn.Conv1d(dim_in, dim_conv, 4, padding=5)  # 32\n",
    "        self.conv1_5 = nn.Conv1d(dim_in, dim_conv, 5, padding=5)  # 31\n",
    "        self.bn = nn.BatchNorm1d(dim_conv * 3)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.maxpool_1 = nn.MaxPool1d(kernel_size=max_length+8)\n",
    "        self.maxpool_2 = nn.MaxPool1d(kernel_size=max_length+7)\n",
    "        self.maxpool_3 = nn.MaxPool1d(kernel_size=max_length+6)\n",
    "        self.fc = nn.Linear(dim_conv * 3, dim_out)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1_3(x)\n",
    "        x1 = self.ReLU(x1)\n",
    "        x1 = self.maxpool_1(x1)\n",
    "\n",
    "\n",
    "        x2 = self.conv1_4(x)\n",
    "        x2 = self.ReLU(x2)\n",
    "        x2 = self.maxpool_2(x2)\n",
    "\n",
    "        x3 = self.conv1_5(x)\n",
    "        x3 = self.ReLU(x3)\n",
    "        x3 = self.maxpool_3(x3)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "dim_in = 25\n",
    "dim_conv = 100\n",
    "dim_out = label_num\n",
    "lr = 0.001\n",
    "model = CNN(dim_in, dim_conv, dim_out)\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss, correct_num = 0, 0\n",
    "    model.train()\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X.size(0)\n",
    "        correct_num += (torch.eq(torch.argmax(pred, dim=1), y)).type(torch.float).sum().item()\n",
    "\n",
    "    train_loss /= size\n",
    "    train_acc = correct_num / size\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct_num = 0, 0\n",
    "    model.eval()  # inform no dropout and fix bn during testing\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item() * X.size(0)\n",
    "            correct_num += (torch.eq(torch.argmax(pred, dim=1), y)).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    test_acc = correct_num / size\n",
    "    return test_loss, test_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss 2.484124 train_acc 0.115000, test_loss 2.487494, test_acc 0.080000\n",
      "Epoch 2, train_loss 2.473181 train_acc 0.115000, test_loss 2.489590, test_acc 0.080000\n",
      "Epoch 3, train_loss 2.475293 train_acc 0.120000, test_loss 2.490917, test_acc 0.080000\n",
      "Epoch 4, train_loss 2.465861 train_acc 0.125000, test_loss 2.492034, test_acc 0.060000\n",
      "Epoch 5, train_loss 2.458362 train_acc 0.165000, test_loss 2.492848, test_acc 0.080000\n",
      "Epoch 6, train_loss 2.462922 train_acc 0.110000, test_loss 2.492199, test_acc 0.100000\n",
      "Epoch 7, train_loss 2.462324 train_acc 0.105000, test_loss 2.491209, test_acc 0.100000\n",
      "Epoch 8, train_loss 2.458241 train_acc 0.140000, test_loss 2.490172, test_acc 0.040000\n",
      "Epoch 9, train_loss 2.430703 train_acc 0.270000, test_loss 2.489311, test_acc 0.060000\n",
      "Epoch 10, train_loss 2.424370 train_acc 0.255000, test_loss 2.488974, test_acc 0.040000\n",
      "Epoch 11, train_loss 2.449313 train_acc 0.190000, test_loss 2.489041, test_acc 0.080000\n",
      "Epoch 12, train_loss 2.397477 train_acc 0.275000, test_loss 2.488865, test_acc 0.040000\n",
      "Epoch 13, train_loss 2.408255 train_acc 0.250000, test_loss 2.488501, test_acc 0.060000\n",
      "Epoch 14, train_loss 2.410165 train_acc 0.290000, test_loss 2.488255, test_acc 0.100000\n",
      "Epoch 15, train_loss 2.412799 train_acc 0.265000, test_loss 2.487189, test_acc 0.080000\n",
      "Epoch 16, train_loss 2.400651 train_acc 0.245000, test_loss 2.485921, test_acc 0.120000\n",
      "Epoch 17, train_loss 2.407674 train_acc 0.275000, test_loss 2.484956, test_acc 0.140000\n",
      "Epoch 18, train_loss 2.396764 train_acc 0.270000, test_loss 2.484226, test_acc 0.120000\n",
      "Epoch 19, train_loss 2.372903 train_acc 0.290000, test_loss 2.485106, test_acc 0.040000\n",
      "Epoch 20, train_loss 2.371848 train_acc 0.280000, test_loss 2.484294, test_acc 0.040000\n",
      "Epoch 21, train_loss 2.384917 train_acc 0.270000, test_loss 2.483366, test_acc 0.020000\n",
      "Epoch 22, train_loss 2.370388 train_acc 0.300000, test_loss 2.480539, test_acc 0.020000\n",
      "Epoch 23, train_loss 2.351341 train_acc 0.345000, test_loss 2.478851, test_acc 0.100000\n",
      "Epoch 24, train_loss 2.337616 train_acc 0.385000, test_loss 2.477389, test_acc 0.160000\n",
      "Epoch 25, train_loss 2.375206 train_acc 0.300000, test_loss 2.477232, test_acc 0.160000\n",
      "Epoch 26, train_loss 2.330910 train_acc 0.375000, test_loss 2.477030, test_acc 0.100000\n",
      "Epoch 27, train_loss 2.340834 train_acc 0.370000, test_loss 2.479092, test_acc 0.120000\n",
      "Epoch 28, train_loss 2.325075 train_acc 0.355000, test_loss 2.481593, test_acc 0.100000\n",
      "Epoch 29, train_loss 2.307248 train_acc 0.375000, test_loss 2.481007, test_acc 0.100000\n",
      "Epoch 30, train_loss 2.284701 train_acc 0.420000, test_loss 2.478865, test_acc 0.100000\n",
      "Epoch 31, train_loss 2.312271 train_acc 0.360000, test_loss 2.476686, test_acc 0.080000\n",
      "Epoch 32, train_loss 2.300593 train_acc 0.375000, test_loss 2.474593, test_acc 0.040000\n",
      "Epoch 33, train_loss 2.306775 train_acc 0.365000, test_loss 2.473512, test_acc 0.040000\n",
      "Epoch 34, train_loss 2.300334 train_acc 0.385000, test_loss 2.474554, test_acc 0.040000\n",
      "Epoch 35, train_loss 2.294196 train_acc 0.400000, test_loss 2.475156, test_acc 0.060000\n",
      "Epoch 36, train_loss 2.259089 train_acc 0.430000, test_loss 2.475028, test_acc 0.160000\n",
      "Epoch 37, train_loss 2.270183 train_acc 0.400000, test_loss 2.473061, test_acc 0.140000\n",
      "Epoch 38, train_loss 2.273435 train_acc 0.390000, test_loss 2.472381, test_acc 0.100000\n",
      "Epoch 39, train_loss 2.244274 train_acc 0.435000, test_loss 2.472329, test_acc 0.100000\n",
      "Epoch 40, train_loss 2.301895 train_acc 0.340000, test_loss 2.471436, test_acc 0.160000\n",
      "Epoch 41, train_loss 2.260468 train_acc 0.400000, test_loss 2.471328, test_acc 0.160000\n",
      "Epoch 42, train_loss 2.288752 train_acc 0.355000, test_loss 2.472315, test_acc 0.120000\n",
      "Epoch 43, train_loss 2.237515 train_acc 0.420000, test_loss 2.472731, test_acc 0.100000\n",
      "Epoch 44, train_loss 2.201603 train_acc 0.455000, test_loss 2.473545, test_acc 0.120000\n",
      "Epoch 45, train_loss 2.212309 train_acc 0.445000, test_loss 2.474192, test_acc 0.100000\n",
      "Epoch 46, train_loss 2.256209 train_acc 0.375000, test_loss 2.474125, test_acc 0.100000\n",
      "Epoch 47, train_loss 2.257299 train_acc 0.370000, test_loss 2.472541, test_acc 0.140000\n",
      "Epoch 48, train_loss 2.197491 train_acc 0.445000, test_loss 2.470303, test_acc 0.160000\n",
      "Epoch 49, train_loss 2.196766 train_acc 0.430000, test_loss 2.469227, test_acc 0.120000\n",
      "Epoch 50, train_loss 2.223631 train_acc 0.395000, test_loss 2.470515, test_acc 0.080000\n",
      "Epoch 51, train_loss 2.230537 train_acc 0.390000, test_loss 2.472203, test_acc 0.080000\n",
      "Epoch 52, train_loss 2.189275 train_acc 0.435000, test_loss 2.472173, test_acc 0.080000\n",
      "Epoch 53, train_loss 2.194114 train_acc 0.430000, test_loss 2.471119, test_acc 0.080000\n",
      "Epoch 54, train_loss 2.231513 train_acc 0.380000, test_loss 2.470331, test_acc 0.100000\n",
      "Epoch 55, train_loss 2.212485 train_acc 0.390000, test_loss 2.470884, test_acc 0.140000\n",
      "Epoch 56, train_loss 2.186267 train_acc 0.420000, test_loss 2.472174, test_acc 0.160000\n",
      "Epoch 57, train_loss 2.208516 train_acc 0.395000, test_loss 2.472016, test_acc 0.140000\n",
      "Epoch 58, train_loss 2.224222 train_acc 0.360000, test_loss 2.470275, test_acc 0.100000\n",
      "Epoch 59, train_loss 2.222156 train_acc 0.365000, test_loss 2.467241, test_acc 0.180000\n",
      "Epoch 60, train_loss 2.148731 train_acc 0.470000, test_loss 2.466934, test_acc 0.120000\n",
      "Epoch 61, train_loss 2.153147 train_acc 0.465000, test_loss 2.467807, test_acc 0.180000\n",
      "Epoch 62, train_loss 2.175328 train_acc 0.425000, test_loss 2.468521, test_acc 0.160000\n",
      "Epoch 63, train_loss 2.185338 train_acc 0.415000, test_loss 2.469898, test_acc 0.140000\n",
      "Epoch 64, train_loss 2.194166 train_acc 0.390000, test_loss 2.470729, test_acc 0.140000\n",
      "Epoch 65, train_loss 2.140654 train_acc 0.455000, test_loss 2.469571, test_acc 0.080000\n",
      "Epoch 66, train_loss 2.201975 train_acc 0.365000, test_loss 2.467598, test_acc 0.140000\n",
      "Epoch 67, train_loss 2.134538 train_acc 0.475000, test_loss 2.465767, test_acc 0.200000\n",
      "Epoch 68, train_loss 2.146717 train_acc 0.450000, test_loss 2.465794, test_acc 0.160000\n",
      "Epoch 69, train_loss 2.175644 train_acc 0.395000, test_loss 2.467266, test_acc 0.140000\n",
      "Epoch 70, train_loss 2.137256 train_acc 0.445000, test_loss 2.468782, test_acc 0.140000\n",
      "Epoch 71, train_loss 2.145753 train_acc 0.435000, test_loss 2.469925, test_acc 0.160000\n",
      "Epoch 72, train_loss 2.171936 train_acc 0.400000, test_loss 2.469420, test_acc 0.180000\n",
      "Epoch 73, train_loss 2.141162 train_acc 0.440000, test_loss 2.467031, test_acc 0.160000\n",
      "Epoch 74, train_loss 2.131616 train_acc 0.460000, test_loss 2.465788, test_acc 0.160000\n",
      "Epoch 75, train_loss 2.151582 train_acc 0.435000, test_loss 2.465835, test_acc 0.120000\n",
      "Epoch 76, train_loss 2.138339 train_acc 0.450000, test_loss 2.467577, test_acc 0.080000\n",
      "Epoch 77, train_loss 2.127282 train_acc 0.470000, test_loss 2.469159, test_acc 0.100000\n",
      "Epoch 78, train_loss 2.137461 train_acc 0.455000, test_loss 2.468540, test_acc 0.140000\n",
      "Epoch 79, train_loss 2.177339 train_acc 0.385000, test_loss 2.465967, test_acc 0.120000\n",
      "Epoch 80, train_loss 2.111606 train_acc 0.485000, test_loss 2.463976, test_acc 0.120000\n",
      "Epoch 81, train_loss 2.154119 train_acc 0.455000, test_loss 2.463872, test_acc 0.180000\n",
      "Epoch 82, train_loss 2.122156 train_acc 0.470000, test_loss 2.464842, test_acc 0.160000\n",
      "Epoch 83, train_loss 2.165577 train_acc 0.435000, test_loss 2.466630, test_acc 0.100000\n",
      "Epoch 84, train_loss 2.105723 train_acc 0.490000, test_loss 2.468376, test_acc 0.100000\n",
      "Epoch 85, train_loss 2.184155 train_acc 0.410000, test_loss 2.469360, test_acc 0.100000\n",
      "Epoch 86, train_loss 2.106325 train_acc 0.500000, test_loss 2.468329, test_acc 0.100000\n",
      "Epoch 87, train_loss 2.148389 train_acc 0.435000, test_loss 2.466412, test_acc 0.140000\n",
      "Epoch 88, train_loss 2.118174 train_acc 0.480000, test_loss 2.464384, test_acc 0.160000\n",
      "Epoch 89, train_loss 2.092715 train_acc 0.515000, test_loss 2.462384, test_acc 0.160000\n",
      "Epoch 90, train_loss 2.136820 train_acc 0.455000, test_loss 2.461896, test_acc 0.180000\n",
      "Epoch 91, train_loss 2.099445 train_acc 0.510000, test_loss 2.463389, test_acc 0.180000\n",
      "Epoch 92, train_loss 2.140505 train_acc 0.440000, test_loss 2.464190, test_acc 0.180000\n",
      "Epoch 93, train_loss 2.127040 train_acc 0.465000, test_loss 2.465163, test_acc 0.180000\n",
      "Epoch 94, train_loss 2.124778 train_acc 0.465000, test_loss 2.466330, test_acc 0.160000\n",
      "Epoch 95, train_loss 2.152648 train_acc 0.435000, test_loss 2.467519, test_acc 0.180000\n",
      "Epoch 96, train_loss 2.123766 train_acc 0.475000, test_loss 2.467626, test_acc 0.160000\n",
      "Epoch 97, train_loss 2.152020 train_acc 0.450000, test_loss 2.466645, test_acc 0.140000\n",
      "Epoch 98, train_loss 2.149961 train_acc 0.435000, test_loss 2.465663, test_acc 0.140000\n",
      "Epoch 99, train_loss 2.121428 train_acc 0.495000, test_loss 2.464925, test_acc 0.160000\n",
      "Epoch 100, train_loss 2.153394 train_acc 0.440000, test_loss 2.464666, test_acc 0.160000\n",
      "Epoch 101, train_loss 2.087687 train_acc 0.515000, test_loss 2.464788, test_acc 0.200000\n",
      "Epoch 102, train_loss 2.113023 train_acc 0.480000, test_loss 2.465991, test_acc 0.160000\n",
      "Epoch 103, train_loss 2.087708 train_acc 0.515000, test_loss 2.467059, test_acc 0.180000\n",
      "Epoch 104, train_loss 2.104376 train_acc 0.490000, test_loss 2.467571, test_acc 0.180000\n",
      "Epoch 105, train_loss 2.112027 train_acc 0.485000, test_loss 2.467682, test_acc 0.180000\n",
      "Epoch 106, train_loss 2.133034 train_acc 0.480000, test_loss 2.467460, test_acc 0.180000\n",
      "Epoch 107, train_loss 2.096525 train_acc 0.500000, test_loss 2.465775, test_acc 0.160000\n",
      "Epoch 108, train_loss 2.083190 train_acc 0.525000, test_loss 2.463774, test_acc 0.160000\n",
      "Epoch 109, train_loss 2.133377 train_acc 0.480000, test_loss 2.462663, test_acc 0.140000\n",
      "Epoch 110, train_loss 2.083041 train_acc 0.550000, test_loss 2.462116, test_acc 0.160000\n",
      "Epoch 111, train_loss 2.168630 train_acc 0.440000, test_loss 2.462427, test_acc 0.140000\n",
      "Epoch 112, train_loss 2.109811 train_acc 0.490000, test_loss 2.463492, test_acc 0.140000\n",
      "Epoch 113, train_loss 2.118520 train_acc 0.475000, test_loss 2.464064, test_acc 0.140000\n",
      "Epoch 114, train_loss 2.113706 train_acc 0.480000, test_loss 2.464136, test_acc 0.180000\n",
      "Epoch 115, train_loss 2.128803 train_acc 0.475000, test_loss 2.463931, test_acc 0.180000\n",
      "Epoch 116, train_loss 2.120835 train_acc 0.505000, test_loss 2.464186, test_acc 0.160000\n",
      "Epoch 117, train_loss 2.109247 train_acc 0.505000, test_loss 2.464708, test_acc 0.160000\n",
      "Epoch 118, train_loss 2.120498 train_acc 0.480000, test_loss 2.465209, test_acc 0.180000\n",
      "Epoch 119, train_loss 2.098885 train_acc 0.525000, test_loss 2.466692, test_acc 0.160000\n",
      "Epoch 120, train_loss 2.127954 train_acc 0.490000, test_loss 2.467670, test_acc 0.140000\n",
      "Epoch 121, train_loss 2.103855 train_acc 0.500000, test_loss 2.468568, test_acc 0.160000\n",
      "Epoch 122, train_loss 2.115228 train_acc 0.475000, test_loss 2.466923, test_acc 0.160000\n",
      "Epoch 123, train_loss 2.072501 train_acc 0.565000, test_loss 2.464921, test_acc 0.160000\n",
      "Epoch 124, train_loss 2.122924 train_acc 0.475000, test_loss 2.463847, test_acc 0.160000\n",
      "Epoch 125, train_loss 2.115972 train_acc 0.515000, test_loss 2.463364, test_acc 0.160000\n",
      "Epoch 126, train_loss 2.087947 train_acc 0.540000, test_loss 2.463607, test_acc 0.160000\n",
      "Epoch 127, train_loss 2.099396 train_acc 0.500000, test_loss 2.464462, test_acc 0.180000\n",
      "Epoch 128, train_loss 2.109891 train_acc 0.485000, test_loss 2.466221, test_acc 0.140000\n",
      "Epoch 129, train_loss 2.142012 train_acc 0.430000, test_loss 2.466657, test_acc 0.140000\n",
      "Epoch 130, train_loss 2.140544 train_acc 0.455000, test_loss 2.465451, test_acc 0.140000\n",
      "Epoch 131, train_loss 2.129411 train_acc 0.475000, test_loss 2.464465, test_acc 0.160000\n",
      "Epoch 132, train_loss 2.134086 train_acc 0.450000, test_loss 2.463399, test_acc 0.140000\n",
      "Epoch 133, train_loss 2.107365 train_acc 0.510000, test_loss 2.462512, test_acc 0.140000\n",
      "Epoch 134, train_loss 2.167967 train_acc 0.430000, test_loss 2.461468, test_acc 0.140000\n",
      "Epoch 135, train_loss 2.109197 train_acc 0.490000, test_loss 2.461338, test_acc 0.140000\n",
      "Epoch 136, train_loss 2.101235 train_acc 0.530000, test_loss 2.461075, test_acc 0.140000\n",
      "Epoch 137, train_loss 2.095229 train_acc 0.520000, test_loss 2.461100, test_acc 0.140000\n",
      "Epoch 138, train_loss 2.112831 train_acc 0.475000, test_loss 2.460722, test_acc 0.120000\n",
      "Epoch 139, train_loss 2.089891 train_acc 0.485000, test_loss 2.461040, test_acc 0.140000\n",
      "Epoch 140, train_loss 2.109011 train_acc 0.540000, test_loss 2.461949, test_acc 0.140000\n",
      "Epoch 141, train_loss 2.081652 train_acc 0.510000, test_loss 2.462790, test_acc 0.140000\n",
      "Epoch 142, train_loss 2.082038 train_acc 0.545000, test_loss 2.463917, test_acc 0.140000\n",
      "Epoch 143, train_loss 2.094074 train_acc 0.545000, test_loss 2.464882, test_acc 0.140000\n",
      "Epoch 144, train_loss 2.104350 train_acc 0.500000, test_loss 2.464495, test_acc 0.160000\n",
      "Epoch 145, train_loss 2.152980 train_acc 0.445000, test_loss 2.463475, test_acc 0.160000\n",
      "Epoch 146, train_loss 2.179365 train_acc 0.430000, test_loss 2.462935, test_acc 0.120000\n",
      "Epoch 147, train_loss 2.119050 train_acc 0.480000, test_loss 2.463299, test_acc 0.140000\n",
      "Epoch 148, train_loss 2.019090 train_acc 0.585000, test_loss 2.463562, test_acc 0.120000\n",
      "Epoch 149, train_loss 2.097736 train_acc 0.525000, test_loss 2.462800, test_acc 0.140000\n",
      "Epoch 150, train_loss 2.075218 train_acc 0.540000, test_loss 2.462347, test_acc 0.120000\n",
      "Epoch 151, train_loss 2.081646 train_acc 0.525000, test_loss 2.462383, test_acc 0.140000\n",
      "Epoch 152, train_loss 2.095091 train_acc 0.515000, test_loss 2.461504, test_acc 0.140000\n",
      "Epoch 153, train_loss 2.090802 train_acc 0.525000, test_loss 2.460773, test_acc 0.160000\n",
      "Epoch 154, train_loss 2.109431 train_acc 0.495000, test_loss 2.460886, test_acc 0.160000\n",
      "Epoch 155, train_loss 2.043079 train_acc 0.565000, test_loss 2.461740, test_acc 0.160000\n",
      "Epoch 156, train_loss 2.098791 train_acc 0.505000, test_loss 2.462691, test_acc 0.160000\n",
      "Epoch 157, train_loss 2.068163 train_acc 0.545000, test_loss 2.464286, test_acc 0.160000\n",
      "Epoch 158, train_loss 2.065370 train_acc 0.575000, test_loss 2.465275, test_acc 0.160000\n",
      "Epoch 159, train_loss 2.084209 train_acc 0.515000, test_loss 2.465024, test_acc 0.140000\n",
      "Epoch 160, train_loss 2.067015 train_acc 0.560000, test_loss 2.465005, test_acc 0.140000\n",
      "Epoch 161, train_loss 2.088004 train_acc 0.520000, test_loss 2.465178, test_acc 0.160000\n",
      "Epoch 162, train_loss 2.024565 train_acc 0.605000, test_loss 2.465140, test_acc 0.160000\n",
      "Epoch 163, train_loss 2.097842 train_acc 0.515000, test_loss 2.465258, test_acc 0.160000\n",
      "Epoch 164, train_loss 2.077222 train_acc 0.550000, test_loss 2.465327, test_acc 0.140000\n",
      "Epoch 165, train_loss 2.080454 train_acc 0.540000, test_loss 2.465510, test_acc 0.140000\n",
      "Epoch 166, train_loss 2.023471 train_acc 0.595000, test_loss 2.465635, test_acc 0.140000\n",
      "Epoch 167, train_loss 2.062155 train_acc 0.535000, test_loss 2.465617, test_acc 0.120000\n",
      "Epoch 168, train_loss 2.072172 train_acc 0.545000, test_loss 2.466497, test_acc 0.140000\n",
      "Epoch 169, train_loss 2.129059 train_acc 0.445000, test_loss 2.467921, test_acc 0.140000\n",
      "Epoch 170, train_loss 2.087002 train_acc 0.510000, test_loss 2.469032, test_acc 0.140000\n",
      "Epoch 171, train_loss 2.043643 train_acc 0.570000, test_loss 2.470588, test_acc 0.140000\n",
      "Epoch 172, train_loss 2.082433 train_acc 0.515000, test_loss 2.475049, test_acc 0.080000\n",
      "Epoch 173, train_loss 2.056394 train_acc 0.540000, test_loss 2.479752, test_acc 0.060000\n",
      "Epoch 174, train_loss 2.086589 train_acc 0.500000, test_loss 2.473618, test_acc 0.080000\n",
      "Epoch 175, train_loss 2.040516 train_acc 0.580000, test_loss 2.471035, test_acc 0.100000\n",
      "Epoch 176, train_loss 2.090087 train_acc 0.510000, test_loss 2.468866, test_acc 0.140000\n",
      "Epoch 177, train_loss 2.065693 train_acc 0.560000, test_loss 2.467941, test_acc 0.140000\n",
      "Epoch 178, train_loss 2.084412 train_acc 0.525000, test_loss 2.469427, test_acc 0.140000\n",
      "Epoch 179, train_loss 2.059535 train_acc 0.545000, test_loss 2.474466, test_acc 0.120000\n",
      "Epoch 180, train_loss 2.106436 train_acc 0.515000, test_loss 2.475537, test_acc 0.120000\n",
      "Epoch 181, train_loss 2.049466 train_acc 0.540000, test_loss 2.472637, test_acc 0.100000\n",
      "Epoch 182, train_loss 2.088133 train_acc 0.515000, test_loss 2.468752, test_acc 0.120000\n",
      "Epoch 183, train_loss 2.074430 train_acc 0.520000, test_loss 2.465812, test_acc 0.160000\n",
      "Epoch 184, train_loss 2.073542 train_acc 0.540000, test_loss 2.466010, test_acc 0.180000\n",
      "Epoch 185, train_loss 2.024111 train_acc 0.580000, test_loss 2.468089, test_acc 0.160000\n",
      "Epoch 186, train_loss 2.084575 train_acc 0.525000, test_loss 2.472615, test_acc 0.100000\n",
      "Epoch 187, train_loss 2.045085 train_acc 0.540000, test_loss 2.475069, test_acc 0.100000\n",
      "Epoch 188, train_loss 2.082843 train_acc 0.510000, test_loss 2.475301, test_acc 0.120000\n",
      "Epoch 189, train_loss 2.045608 train_acc 0.555000, test_loss 2.471575, test_acc 0.120000\n",
      "Epoch 190, train_loss 2.122198 train_acc 0.505000, test_loss 2.468408, test_acc 0.160000\n",
      "Epoch 191, train_loss 2.063545 train_acc 0.530000, test_loss 2.467983, test_acc 0.160000\n",
      "Epoch 192, train_loss 2.028566 train_acc 0.555000, test_loss 2.468640, test_acc 0.140000\n",
      "Epoch 193, train_loss 2.064855 train_acc 0.530000, test_loss 2.469622, test_acc 0.100000\n",
      "Epoch 194, train_loss 2.061516 train_acc 0.540000, test_loss 2.467932, test_acc 0.120000\n",
      "Epoch 195, train_loss 2.096662 train_acc 0.520000, test_loss 2.467190, test_acc 0.120000\n",
      "Epoch 196, train_loss 2.041588 train_acc 0.575000, test_loss 2.467431, test_acc 0.120000\n",
      "Epoch 197, train_loss 2.024137 train_acc 0.585000, test_loss 2.467311, test_acc 0.120000\n",
      "Epoch 198, train_loss 2.065604 train_acc 0.545000, test_loss 2.468359, test_acc 0.120000\n",
      "Epoch 199, train_loss 2.019875 train_acc 0.590000, test_loss 2.469533, test_acc 0.140000\n",
      "Epoch 200, train_loss 2.050016 train_acc 0.545000, test_loss 2.470419, test_acc 0.140000\n",
      "Epoch 201, train_loss 2.117364 train_acc 0.465000, test_loss 2.469482, test_acc 0.140000\n",
      "Epoch 202, train_loss 2.081735 train_acc 0.495000, test_loss 2.467753, test_acc 0.120000\n",
      "Epoch 203, train_loss 2.052463 train_acc 0.530000, test_loss 2.466560, test_acc 0.120000\n",
      "Epoch 204, train_loss 2.027600 train_acc 0.580000, test_loss 2.466262, test_acc 0.120000\n",
      "Epoch 205, train_loss 2.078897 train_acc 0.530000, test_loss 2.466865, test_acc 0.140000\n",
      "Epoch 206, train_loss 2.016678 train_acc 0.620000, test_loss 2.468827, test_acc 0.140000\n",
      "Epoch 207, train_loss 2.095218 train_acc 0.510000, test_loss 2.470362, test_acc 0.120000\n",
      "Epoch 208, train_loss 2.053773 train_acc 0.555000, test_loss 2.470443, test_acc 0.120000\n",
      "Epoch 209, train_loss 2.103733 train_acc 0.485000, test_loss 2.468974, test_acc 0.140000\n",
      "Epoch 210, train_loss 2.079893 train_acc 0.540000, test_loss 2.467616, test_acc 0.140000\n",
      "Epoch 211, train_loss 2.100238 train_acc 0.495000, test_loss 2.466639, test_acc 0.140000\n",
      "Epoch 212, train_loss 2.094245 train_acc 0.555000, test_loss 2.466502, test_acc 0.140000\n",
      "Epoch 213, train_loss 2.074129 train_acc 0.515000, test_loss 2.467297, test_acc 0.140000\n",
      "Epoch 214, train_loss 2.070535 train_acc 0.525000, test_loss 2.468648, test_acc 0.140000\n",
      "Epoch 215, train_loss 2.060049 train_acc 0.545000, test_loss 2.469704, test_acc 0.140000\n",
      "Epoch 216, train_loss 2.047421 train_acc 0.540000, test_loss 2.469383, test_acc 0.140000\n",
      "Epoch 217, train_loss 2.048154 train_acc 0.560000, test_loss 2.468621, test_acc 0.140000\n",
      "Epoch 218, train_loss 2.020328 train_acc 0.570000, test_loss 2.467822, test_acc 0.140000\n",
      "Epoch 219, train_loss 2.058949 train_acc 0.530000, test_loss 2.467474, test_acc 0.140000\n",
      "Epoch 220, train_loss 2.073538 train_acc 0.510000, test_loss 2.467299, test_acc 0.140000\n",
      "Epoch 221, train_loss 2.112832 train_acc 0.490000, test_loss 2.467374, test_acc 0.140000\n",
      "Epoch 222, train_loss 2.042992 train_acc 0.550000, test_loss 2.467609, test_acc 0.140000\n",
      "Epoch 223, train_loss 2.073836 train_acc 0.515000, test_loss 2.467720, test_acc 0.140000\n",
      "Epoch 224, train_loss 2.106428 train_acc 0.475000, test_loss 2.467354, test_acc 0.140000\n",
      "Epoch 225, train_loss 2.046155 train_acc 0.555000, test_loss 2.466582, test_acc 0.140000\n",
      "Epoch 226, train_loss 2.018076 train_acc 0.565000, test_loss 2.466221, test_acc 0.140000\n",
      "Epoch 227, train_loss 2.001472 train_acc 0.615000, test_loss 2.466514, test_acc 0.160000\n",
      "Epoch 228, train_loss 2.065975 train_acc 0.530000, test_loss 2.466625, test_acc 0.140000\n",
      "Epoch 229, train_loss 2.025053 train_acc 0.575000, test_loss 2.466794, test_acc 0.140000\n",
      "Epoch 230, train_loss 2.012715 train_acc 0.590000, test_loss 2.467051, test_acc 0.140000\n",
      "Epoch 231, train_loss 2.065857 train_acc 0.510000, test_loss 2.467914, test_acc 0.140000\n",
      "Epoch 232, train_loss 2.072166 train_acc 0.530000, test_loss 2.468657, test_acc 0.140000\n",
      "Epoch 233, train_loss 2.057077 train_acc 0.555000, test_loss 2.468594, test_acc 0.140000\n",
      "Epoch 234, train_loss 2.049274 train_acc 0.535000, test_loss 2.468102, test_acc 0.140000\n",
      "Epoch 235, train_loss 2.015147 train_acc 0.620000, test_loss 2.467825, test_acc 0.140000\n",
      "Epoch 236, train_loss 2.043341 train_acc 0.590000, test_loss 2.467873, test_acc 0.140000\n",
      "Epoch 237, train_loss 2.045837 train_acc 0.550000, test_loss 2.467465, test_acc 0.140000\n",
      "Epoch 238, train_loss 2.033321 train_acc 0.545000, test_loss 2.466825, test_acc 0.180000\n",
      "Epoch 239, train_loss 2.058904 train_acc 0.545000, test_loss 2.466169, test_acc 0.180000\n",
      "Epoch 240, train_loss 2.061837 train_acc 0.545000, test_loss 2.466015, test_acc 0.180000\n",
      "Epoch 241, train_loss 2.039345 train_acc 0.575000, test_loss 2.466321, test_acc 0.180000\n",
      "Epoch 242, train_loss 2.074919 train_acc 0.525000, test_loss 2.466873, test_acc 0.140000\n",
      "Epoch 243, train_loss 2.043255 train_acc 0.560000, test_loss 2.467249, test_acc 0.140000\n",
      "Epoch 244, train_loss 2.053377 train_acc 0.530000, test_loss 2.467349, test_acc 0.140000\n",
      "Epoch 245, train_loss 2.055819 train_acc 0.545000, test_loss 2.467172, test_acc 0.140000\n",
      "Epoch 246, train_loss 2.008110 train_acc 0.590000, test_loss 2.467330, test_acc 0.140000\n",
      "Epoch 247, train_loss 2.011964 train_acc 0.580000, test_loss 2.467890, test_acc 0.140000\n",
      "Epoch 248, train_loss 2.018419 train_acc 0.585000, test_loss 2.468243, test_acc 0.140000\n",
      "Epoch 249, train_loss 2.047922 train_acc 0.540000, test_loss 2.467830, test_acc 0.140000\n",
      "Epoch 250, train_loss 2.028195 train_acc 0.575000, test_loss 2.467278, test_acc 0.140000\n",
      "Epoch 251, train_loss 2.046291 train_acc 0.550000, test_loss 2.467002, test_acc 0.140000\n",
      "Epoch 252, train_loss 2.056482 train_acc 0.550000, test_loss 2.466737, test_acc 0.180000\n",
      "Epoch 253, train_loss 2.092545 train_acc 0.495000, test_loss 2.466797, test_acc 0.180000\n",
      "Epoch 254, train_loss 2.024116 train_acc 0.590000, test_loss 2.467225, test_acc 0.160000\n",
      "Epoch 255, train_loss 2.041784 train_acc 0.600000, test_loss 2.468065, test_acc 0.140000\n",
      "Epoch 256, train_loss 2.042247 train_acc 0.545000, test_loss 2.468508, test_acc 0.140000\n",
      "Epoch 257, train_loss 2.028983 train_acc 0.575000, test_loss 2.468945, test_acc 0.140000\n",
      "Epoch 258, train_loss 2.040953 train_acc 0.565000, test_loss 2.468760, test_acc 0.140000\n",
      "Epoch 259, train_loss 2.080150 train_acc 0.520000, test_loss 2.468144, test_acc 0.140000\n",
      "Epoch 260, train_loss 2.039901 train_acc 0.555000, test_loss 2.467598, test_acc 0.140000\n",
      "Epoch 261, train_loss 2.036989 train_acc 0.570000, test_loss 2.467131, test_acc 0.140000\n",
      "Epoch 262, train_loss 2.036443 train_acc 0.600000, test_loss 2.467283, test_acc 0.140000\n",
      "Epoch 263, train_loss 2.029565 train_acc 0.570000, test_loss 2.467734, test_acc 0.140000\n",
      "Epoch 264, train_loss 2.068168 train_acc 0.540000, test_loss 2.468121, test_acc 0.140000\n",
      "Epoch 265, train_loss 2.068995 train_acc 0.540000, test_loss 2.468635, test_acc 0.140000\n",
      "Epoch 266, train_loss 2.100598 train_acc 0.460000, test_loss 2.469316, test_acc 0.140000\n",
      "Epoch 267, train_loss 2.062271 train_acc 0.530000, test_loss 2.469693, test_acc 0.140000\n",
      "Epoch 268, train_loss 2.033500 train_acc 0.555000, test_loss 2.469754, test_acc 0.140000\n",
      "Epoch 269, train_loss 2.044059 train_acc 0.560000, test_loss 2.469639, test_acc 0.140000\n",
      "Epoch 270, train_loss 2.064550 train_acc 0.530000, test_loss 2.469104, test_acc 0.140000\n",
      "Epoch 271, train_loss 2.050687 train_acc 0.535000, test_loss 2.468417, test_acc 0.160000\n",
      "Epoch 272, train_loss 2.080939 train_acc 0.505000, test_loss 2.467734, test_acc 0.140000\n",
      "Epoch 273, train_loss 2.095643 train_acc 0.495000, test_loss 2.467555, test_acc 0.140000\n",
      "Epoch 274, train_loss 2.024154 train_acc 0.565000, test_loss 2.467225, test_acc 0.140000\n",
      "Epoch 275, train_loss 1.999994 train_acc 0.605000, test_loss 2.467006, test_acc 0.140000\n",
      "Epoch 276, train_loss 2.020306 train_acc 0.595000, test_loss 2.466660, test_acc 0.140000\n",
      "Epoch 277, train_loss 2.037207 train_acc 0.575000, test_loss 2.466398, test_acc 0.140000\n",
      "Epoch 278, train_loss 2.061369 train_acc 0.555000, test_loss 2.466141, test_acc 0.160000\n",
      "Epoch 279, train_loss 2.038826 train_acc 0.565000, test_loss 2.466349, test_acc 0.160000\n",
      "Epoch 280, train_loss 2.056798 train_acc 0.540000, test_loss 2.466584, test_acc 0.160000\n",
      "Epoch 281, train_loss 2.058182 train_acc 0.535000, test_loss 2.467038, test_acc 0.160000\n",
      "Epoch 282, train_loss 2.102944 train_acc 0.500000, test_loss 2.467731, test_acc 0.160000\n",
      "Epoch 283, train_loss 2.052731 train_acc 0.570000, test_loss 2.468175, test_acc 0.140000\n",
      "Epoch 284, train_loss 2.040422 train_acc 0.545000, test_loss 2.468362, test_acc 0.140000\n",
      "Epoch 285, train_loss 2.024704 train_acc 0.555000, test_loss 2.468222, test_acc 0.140000\n",
      "Epoch 286, train_loss 2.057584 train_acc 0.545000, test_loss 2.467830, test_acc 0.160000\n",
      "Epoch 287, train_loss 2.061693 train_acc 0.525000, test_loss 2.467428, test_acc 0.160000\n",
      "Epoch 288, train_loss 2.031817 train_acc 0.550000, test_loss 2.467055, test_acc 0.180000\n",
      "Epoch 289, train_loss 2.096150 train_acc 0.520000, test_loss 2.466820, test_acc 0.180000\n",
      "Epoch 290, train_loss 2.037669 train_acc 0.580000, test_loss 2.466810, test_acc 0.180000\n",
      "Epoch 291, train_loss 2.031755 train_acc 0.575000, test_loss 2.467017, test_acc 0.160000\n",
      "Epoch 292, train_loss 2.063446 train_acc 0.545000, test_loss 2.467149, test_acc 0.160000\n",
      "Epoch 293, train_loss 2.039380 train_acc 0.565000, test_loss 2.467067, test_acc 0.180000\n",
      "Epoch 294, train_loss 2.023563 train_acc 0.560000, test_loss 2.467150, test_acc 0.180000\n",
      "Epoch 295, train_loss 2.037608 train_acc 0.580000, test_loss 2.467500, test_acc 0.180000\n",
      "Epoch 296, train_loss 2.017451 train_acc 0.560000, test_loss 2.468008, test_acc 0.180000\n",
      "Epoch 297, train_loss 2.050793 train_acc 0.570000, test_loss 2.468634, test_acc 0.160000\n",
      "Epoch 298, train_loss 2.019783 train_acc 0.570000, test_loss 2.469078, test_acc 0.140000\n",
      "Epoch 299, train_loss 2.064829 train_acc 0.525000, test_loss 2.469177, test_acc 0.140000\n",
      "Epoch 300, train_loss 2.062749 train_acc 0.530000, test_loss 2.468976, test_acc 0.140000\n",
      "Epoch 301, train_loss 2.080820 train_acc 0.515000, test_loss 2.468504, test_acc 0.140000\n",
      "Epoch 302, train_loss 2.062001 train_acc 0.535000, test_loss 2.467885, test_acc 0.140000\n",
      "Epoch 303, train_loss 2.050490 train_acc 0.545000, test_loss 2.467170, test_acc 0.140000\n",
      "Epoch 304, train_loss 2.005318 train_acc 0.630000, test_loss 2.466703, test_acc 0.160000\n",
      "Epoch 305, train_loss 2.085036 train_acc 0.500000, test_loss 2.466347, test_acc 0.160000\n",
      "Epoch 306, train_loss 2.001701 train_acc 0.575000, test_loss 2.466284, test_acc 0.160000\n",
      "Epoch 307, train_loss 2.029281 train_acc 0.600000, test_loss 2.466380, test_acc 0.160000\n",
      "Epoch 308, train_loss 2.078882 train_acc 0.495000, test_loss 2.466626, test_acc 0.160000\n",
      "Epoch 309, train_loss 2.086945 train_acc 0.505000, test_loss 2.466847, test_acc 0.160000\n",
      "Epoch 310, train_loss 2.033658 train_acc 0.565000, test_loss 2.467011, test_acc 0.160000\n",
      "Epoch 311, train_loss 2.036934 train_acc 0.565000, test_loss 2.467275, test_acc 0.160000\n",
      "Epoch 312, train_loss 2.048746 train_acc 0.545000, test_loss 2.467111, test_acc 0.160000\n",
      "Epoch 313, train_loss 2.066887 train_acc 0.495000, test_loss 2.466918, test_acc 0.160000\n",
      "Epoch 314, train_loss 2.083535 train_acc 0.520000, test_loss 2.466812, test_acc 0.160000\n",
      "Epoch 315, train_loss 2.059383 train_acc 0.530000, test_loss 2.466771, test_acc 0.160000\n",
      "Epoch 316, train_loss 2.081764 train_acc 0.500000, test_loss 2.466582, test_acc 0.160000\n",
      "Epoch 317, train_loss 2.016937 train_acc 0.565000, test_loss 2.466687, test_acc 0.160000\n",
      "Epoch 318, train_loss 2.054185 train_acc 0.555000, test_loss 2.467046, test_acc 0.160000\n",
      "Epoch 319, train_loss 2.074260 train_acc 0.495000, test_loss 2.467657, test_acc 0.160000\n",
      "Epoch 320, train_loss 2.035452 train_acc 0.565000, test_loss 2.468245, test_acc 0.160000\n",
      "Epoch 321, train_loss 2.047664 train_acc 0.560000, test_loss 2.468708, test_acc 0.140000\n",
      "Epoch 322, train_loss 2.052213 train_acc 0.550000, test_loss 2.469076, test_acc 0.160000\n",
      "Epoch 323, train_loss 1.977637 train_acc 0.610000, test_loss 2.469172, test_acc 0.160000\n",
      "Epoch 324, train_loss 2.037007 train_acc 0.545000, test_loss 2.469227, test_acc 0.160000\n",
      "Epoch 325, train_loss 2.059795 train_acc 0.550000, test_loss 2.469039, test_acc 0.160000\n",
      "Epoch 326, train_loss 2.048690 train_acc 0.555000, test_loss 2.468833, test_acc 0.160000\n",
      "Epoch 327, train_loss 2.034450 train_acc 0.565000, test_loss 2.468687, test_acc 0.160000\n",
      "Epoch 328, train_loss 2.005564 train_acc 0.610000, test_loss 2.468521, test_acc 0.160000\n",
      "Epoch 329, train_loss 2.021676 train_acc 0.600000, test_loss 2.468323, test_acc 0.160000\n",
      "Epoch 330, train_loss 2.031251 train_acc 0.570000, test_loss 2.468041, test_acc 0.160000\n",
      "Epoch 331, train_loss 2.037393 train_acc 0.565000, test_loss 2.467747, test_acc 0.160000\n",
      "Epoch 332, train_loss 2.021920 train_acc 0.565000, test_loss 2.467545, test_acc 0.160000\n",
      "Epoch 333, train_loss 2.051847 train_acc 0.545000, test_loss 2.467478, test_acc 0.160000\n",
      "Epoch 334, train_loss 2.036692 train_acc 0.580000, test_loss 2.467822, test_acc 0.160000\n",
      "Epoch 335, train_loss 2.016332 train_acc 0.585000, test_loss 2.468003, test_acc 0.160000\n",
      "Epoch 336, train_loss 2.012749 train_acc 0.615000, test_loss 2.468250, test_acc 0.160000\n",
      "Epoch 337, train_loss 2.051364 train_acc 0.555000, test_loss 2.468320, test_acc 0.160000\n",
      "Epoch 338, train_loss 2.040822 train_acc 0.545000, test_loss 2.468232, test_acc 0.160000\n",
      "Epoch 339, train_loss 2.006703 train_acc 0.595000, test_loss 2.468069, test_acc 0.180000\n",
      "Epoch 340, train_loss 2.075459 train_acc 0.540000, test_loss 2.467623, test_acc 0.180000\n",
      "Epoch 341, train_loss 2.051990 train_acc 0.505000, test_loss 2.466855, test_acc 0.180000\n",
      "Epoch 342, train_loss 2.100480 train_acc 0.515000, test_loss 2.466336, test_acc 0.180000\n",
      "Epoch 343, train_loss 2.043789 train_acc 0.580000, test_loss 2.466100, test_acc 0.180000\n",
      "Epoch 344, train_loss 2.039188 train_acc 0.565000, test_loss 2.465933, test_acc 0.180000\n",
      "Epoch 345, train_loss 2.079917 train_acc 0.510000, test_loss 2.466106, test_acc 0.180000\n",
      "Epoch 346, train_loss 2.099797 train_acc 0.480000, test_loss 2.466596, test_acc 0.180000\n",
      "Epoch 347, train_loss 2.038805 train_acc 0.570000, test_loss 2.467043, test_acc 0.160000\n",
      "Epoch 348, train_loss 2.066627 train_acc 0.545000, test_loss 2.467566, test_acc 0.160000\n",
      "Epoch 349, train_loss 2.058753 train_acc 0.545000, test_loss 2.467885, test_acc 0.160000\n",
      "Epoch 350, train_loss 2.024206 train_acc 0.585000, test_loss 2.468421, test_acc 0.140000\n",
      "Epoch 351, train_loss 2.040231 train_acc 0.550000, test_loss 2.468966, test_acc 0.140000\n",
      "Epoch 352, train_loss 2.063718 train_acc 0.525000, test_loss 2.469400, test_acc 0.140000\n",
      "Epoch 353, train_loss 2.084596 train_acc 0.510000, test_loss 2.469446, test_acc 0.140000\n",
      "Epoch 354, train_loss 1.984440 train_acc 0.610000, test_loss 2.469228, test_acc 0.140000\n",
      "Epoch 355, train_loss 2.058367 train_acc 0.565000, test_loss 2.469100, test_acc 0.140000\n",
      "Epoch 356, train_loss 2.070894 train_acc 0.530000, test_loss 2.468561, test_acc 0.160000\n",
      "Epoch 357, train_loss 2.078945 train_acc 0.520000, test_loss 2.467776, test_acc 0.160000\n",
      "Epoch 358, train_loss 2.040064 train_acc 0.570000, test_loss 2.467165, test_acc 0.160000\n",
      "Epoch 359, train_loss 2.090202 train_acc 0.490000, test_loss 2.466812, test_acc 0.180000\n",
      "Epoch 360, train_loss 2.066356 train_acc 0.525000, test_loss 2.466858, test_acc 0.180000\n",
      "Epoch 361, train_loss 2.044711 train_acc 0.585000, test_loss 2.467197, test_acc 0.180000\n",
      "Epoch 362, train_loss 2.049783 train_acc 0.555000, test_loss 2.467890, test_acc 0.180000\n",
      "Epoch 363, train_loss 2.072745 train_acc 0.515000, test_loss 2.468628, test_acc 0.160000\n",
      "Epoch 364, train_loss 2.031123 train_acc 0.555000, test_loss 2.469102, test_acc 0.160000\n",
      "Epoch 365, train_loss 2.004078 train_acc 0.595000, test_loss 2.469505, test_acc 0.160000\n",
      "Epoch 366, train_loss 2.060844 train_acc 0.535000, test_loss 2.469839, test_acc 0.160000\n",
      "Epoch 367, train_loss 2.049212 train_acc 0.560000, test_loss 2.470258, test_acc 0.160000\n",
      "Epoch 368, train_loss 2.038764 train_acc 0.575000, test_loss 2.470312, test_acc 0.160000\n",
      "Epoch 369, train_loss 2.042182 train_acc 0.565000, test_loss 2.470238, test_acc 0.160000\n",
      "Epoch 370, train_loss 2.060567 train_acc 0.515000, test_loss 2.469806, test_acc 0.160000\n",
      "Epoch 371, train_loss 2.075225 train_acc 0.530000, test_loss 2.469110, test_acc 0.160000\n",
      "Epoch 372, train_loss 2.104260 train_acc 0.525000, test_loss 2.468519, test_acc 0.160000\n",
      "Epoch 373, train_loss 2.019464 train_acc 0.570000, test_loss 2.468083, test_acc 0.160000\n",
      "Epoch 374, train_loss 2.027001 train_acc 0.570000, test_loss 2.467551, test_acc 0.160000\n",
      "Epoch 375, train_loss 2.044906 train_acc 0.530000, test_loss 2.467227, test_acc 0.160000\n",
      "Epoch 376, train_loss 1.995890 train_acc 0.610000, test_loss 2.467366, test_acc 0.160000\n",
      "Epoch 377, train_loss 2.050325 train_acc 0.535000, test_loss 2.467604, test_acc 0.160000\n",
      "Epoch 378, train_loss 2.051535 train_acc 0.525000, test_loss 2.467833, test_acc 0.160000\n",
      "Epoch 379, train_loss 2.054715 train_acc 0.550000, test_loss 2.468060, test_acc 0.160000\n",
      "Epoch 380, train_loss 2.020908 train_acc 0.565000, test_loss 2.468358, test_acc 0.160000\n",
      "Epoch 381, train_loss 2.054822 train_acc 0.545000, test_loss 2.468675, test_acc 0.160000\n",
      "Epoch 382, train_loss 2.014299 train_acc 0.555000, test_loss 2.468844, test_acc 0.160000\n",
      "Epoch 383, train_loss 2.010944 train_acc 0.595000, test_loss 2.468972, test_acc 0.160000\n",
      "Epoch 384, train_loss 2.052588 train_acc 0.550000, test_loss 2.468890, test_acc 0.160000\n",
      "Epoch 385, train_loss 1.993287 train_acc 0.610000, test_loss 2.468727, test_acc 0.160000\n",
      "Epoch 386, train_loss 2.047820 train_acc 0.550000, test_loss 2.468649, test_acc 0.180000\n",
      "Epoch 387, train_loss 2.035812 train_acc 0.530000, test_loss 2.468624, test_acc 0.180000\n",
      "Epoch 388, train_loss 2.017100 train_acc 0.590000, test_loss 2.468454, test_acc 0.180000\n",
      "Epoch 389, train_loss 2.007358 train_acc 0.580000, test_loss 2.468108, test_acc 0.180000\n",
      "Epoch 390, train_loss 2.031814 train_acc 0.565000, test_loss 2.467880, test_acc 0.180000\n",
      "Epoch 391, train_loss 2.051714 train_acc 0.530000, test_loss 2.467903, test_acc 0.180000\n",
      "Epoch 392, train_loss 2.017434 train_acc 0.600000, test_loss 2.468078, test_acc 0.160000\n",
      "Epoch 393, train_loss 2.072151 train_acc 0.530000, test_loss 2.468142, test_acc 0.160000\n",
      "Epoch 394, train_loss 2.006724 train_acc 0.585000, test_loss 2.468127, test_acc 0.160000\n",
      "Epoch 395, train_loss 2.032645 train_acc 0.555000, test_loss 2.468070, test_acc 0.160000\n",
      "Epoch 396, train_loss 2.080673 train_acc 0.525000, test_loss 2.468025, test_acc 0.160000\n",
      "Epoch 397, train_loss 2.011692 train_acc 0.580000, test_loss 2.467783, test_acc 0.160000\n",
      "Epoch 398, train_loss 2.023293 train_acc 0.575000, test_loss 2.467645, test_acc 0.160000\n",
      "Epoch 399, train_loss 2.047275 train_acc 0.550000, test_loss 2.467655, test_acc 0.160000\n",
      "Epoch 400, train_loss 2.040010 train_acc 0.535000, test_loss 2.467564, test_acc 0.160000\n",
      "Epoch 401, train_loss 2.020577 train_acc 0.590000, test_loss 2.467725, test_acc 0.160000\n",
      "Epoch 402, train_loss 2.048528 train_acc 0.545000, test_loss 2.468040, test_acc 0.160000\n",
      "Epoch 403, train_loss 1.989741 train_acc 0.605000, test_loss 2.468498, test_acc 0.160000\n",
      "Epoch 404, train_loss 2.014588 train_acc 0.590000, test_loss 2.468859, test_acc 0.160000\n",
      "Epoch 405, train_loss 2.029191 train_acc 0.565000, test_loss 2.469003, test_acc 0.160000\n",
      "Epoch 406, train_loss 2.078702 train_acc 0.530000, test_loss 2.469078, test_acc 0.160000\n",
      "Epoch 407, train_loss 2.014825 train_acc 0.565000, test_loss 2.469005, test_acc 0.180000\n",
      "Epoch 408, train_loss 2.002019 train_acc 0.590000, test_loss 2.468993, test_acc 0.180000\n",
      "Epoch 409, train_loss 2.078711 train_acc 0.525000, test_loss 2.468846, test_acc 0.180000\n",
      "Epoch 410, train_loss 2.043893 train_acc 0.565000, test_loss 2.468695, test_acc 0.180000\n",
      "Epoch 411, train_loss 2.025333 train_acc 0.565000, test_loss 2.468607, test_acc 0.180000\n",
      "Epoch 412, train_loss 2.017954 train_acc 0.570000, test_loss 2.468447, test_acc 0.180000\n",
      "Epoch 413, train_loss 2.051613 train_acc 0.535000, test_loss 2.468368, test_acc 0.180000\n",
      "Epoch 414, train_loss 2.045557 train_acc 0.555000, test_loss 2.468473, test_acc 0.180000\n",
      "Epoch 415, train_loss 2.046104 train_acc 0.560000, test_loss 2.468265, test_acc 0.180000\n",
      "Epoch 416, train_loss 2.051684 train_acc 0.505000, test_loss 2.468070, test_acc 0.180000\n",
      "Epoch 417, train_loss 2.046463 train_acc 0.555000, test_loss 2.467904, test_acc 0.180000\n",
      "Epoch 418, train_loss 2.007534 train_acc 0.600000, test_loss 2.467791, test_acc 0.180000\n",
      "Epoch 419, train_loss 2.044048 train_acc 0.550000, test_loss 2.467685, test_acc 0.180000\n",
      "Epoch 420, train_loss 2.046273 train_acc 0.575000, test_loss 2.467648, test_acc 0.180000\n",
      "Epoch 421, train_loss 2.050935 train_acc 0.555000, test_loss 2.467638, test_acc 0.180000\n",
      "Epoch 422, train_loss 2.057455 train_acc 0.530000, test_loss 2.467755, test_acc 0.180000\n",
      "Epoch 423, train_loss 2.087589 train_acc 0.550000, test_loss 2.467825, test_acc 0.180000\n",
      "Epoch 424, train_loss 2.077376 train_acc 0.485000, test_loss 2.468001, test_acc 0.180000\n",
      "Epoch 425, train_loss 2.053504 train_acc 0.530000, test_loss 2.467950, test_acc 0.180000\n",
      "Epoch 426, train_loss 2.076704 train_acc 0.520000, test_loss 2.467668, test_acc 0.180000\n",
      "Epoch 427, train_loss 2.038959 train_acc 0.550000, test_loss 2.467464, test_acc 0.180000\n",
      "Epoch 428, train_loss 2.038104 train_acc 0.555000, test_loss 2.467296, test_acc 0.180000\n",
      "Epoch 429, train_loss 2.092435 train_acc 0.525000, test_loss 2.467212, test_acc 0.180000\n",
      "Epoch 430, train_loss 2.027337 train_acc 0.570000, test_loss 2.467354, test_acc 0.180000\n",
      "Epoch 431, train_loss 2.029008 train_acc 0.560000, test_loss 2.467663, test_acc 0.180000\n",
      "Epoch 432, train_loss 2.024580 train_acc 0.575000, test_loss 2.468096, test_acc 0.180000\n",
      "Epoch 433, train_loss 2.036959 train_acc 0.575000, test_loss 2.468510, test_acc 0.180000\n",
      "Epoch 434, train_loss 2.033336 train_acc 0.575000, test_loss 2.468705, test_acc 0.180000\n",
      "Epoch 435, train_loss 2.075628 train_acc 0.530000, test_loss 2.468879, test_acc 0.180000\n",
      "Epoch 436, train_loss 2.037728 train_acc 0.560000, test_loss 2.468831, test_acc 0.180000\n",
      "Epoch 437, train_loss 2.007792 train_acc 0.585000, test_loss 2.468457, test_acc 0.180000\n",
      "Epoch 438, train_loss 2.017908 train_acc 0.595000, test_loss 2.468086, test_acc 0.180000\n",
      "Epoch 439, train_loss 2.035686 train_acc 0.560000, test_loss 2.467785, test_acc 0.180000\n",
      "Epoch 440, train_loss 2.031362 train_acc 0.580000, test_loss 2.467641, test_acc 0.180000\n",
      "Epoch 441, train_loss 2.028034 train_acc 0.555000, test_loss 2.467509, test_acc 0.180000\n",
      "Epoch 442, train_loss 2.008616 train_acc 0.595000, test_loss 2.467498, test_acc 0.180000\n",
      "Epoch 443, train_loss 2.000116 train_acc 0.615000, test_loss 2.467642, test_acc 0.160000\n",
      "Epoch 444, train_loss 2.036742 train_acc 0.565000, test_loss 2.467918, test_acc 0.160000\n",
      "Epoch 445, train_loss 2.063203 train_acc 0.540000, test_loss 2.468242, test_acc 0.160000\n",
      "Epoch 446, train_loss 2.042607 train_acc 0.570000, test_loss 2.468606, test_acc 0.160000\n",
      "Epoch 447, train_loss 2.037729 train_acc 0.580000, test_loss 2.468765, test_acc 0.160000\n",
      "Epoch 448, train_loss 2.030525 train_acc 0.570000, test_loss 2.468869, test_acc 0.160000\n",
      "Epoch 449, train_loss 2.017948 train_acc 0.570000, test_loss 2.468792, test_acc 0.180000\n",
      "Epoch 450, train_loss 2.052342 train_acc 0.530000, test_loss 2.468814, test_acc 0.180000\n",
      "Epoch 451, train_loss 2.027437 train_acc 0.565000, test_loss 2.468924, test_acc 0.180000\n",
      "Epoch 452, train_loss 2.062487 train_acc 0.515000, test_loss 2.468890, test_acc 0.180000\n",
      "Epoch 453, train_loss 2.023384 train_acc 0.580000, test_loss 2.468765, test_acc 0.180000\n",
      "Epoch 454, train_loss 2.076925 train_acc 0.510000, test_loss 2.468718, test_acc 0.180000\n",
      "Epoch 455, train_loss 2.061045 train_acc 0.535000, test_loss 2.468788, test_acc 0.180000\n",
      "Epoch 456, train_loss 2.027952 train_acc 0.540000, test_loss 2.468987, test_acc 0.180000\n",
      "Epoch 457, train_loss 2.076363 train_acc 0.505000, test_loss 2.469074, test_acc 0.180000\n",
      "Epoch 458, train_loss 2.088031 train_acc 0.525000, test_loss 2.469254, test_acc 0.180000\n",
      "Epoch 459, train_loss 2.032027 train_acc 0.585000, test_loss 2.469504, test_acc 0.160000\n",
      "Epoch 460, train_loss 2.082891 train_acc 0.480000, test_loss 2.469660, test_acc 0.160000\n",
      "Epoch 461, train_loss 2.073598 train_acc 0.535000, test_loss 2.469766, test_acc 0.160000\n",
      "Epoch 462, train_loss 2.078477 train_acc 0.500000, test_loss 2.469625, test_acc 0.160000\n",
      "Epoch 463, train_loss 2.032854 train_acc 0.575000, test_loss 2.469374, test_acc 0.160000\n",
      "Epoch 464, train_loss 2.047063 train_acc 0.560000, test_loss 2.469128, test_acc 0.160000\n",
      "Epoch 465, train_loss 2.016152 train_acc 0.580000, test_loss 2.469065, test_acc 0.160000\n",
      "Epoch 466, train_loss 2.056669 train_acc 0.530000, test_loss 2.469160, test_acc 0.160000\n",
      "Epoch 467, train_loss 2.035986 train_acc 0.570000, test_loss 2.468942, test_acc 0.180000\n",
      "Epoch 468, train_loss 2.025698 train_acc 0.575000, test_loss 2.468497, test_acc 0.180000\n",
      "Epoch 469, train_loss 2.064947 train_acc 0.505000, test_loss 2.468223, test_acc 0.180000\n",
      "Epoch 470, train_loss 2.033128 train_acc 0.560000, test_loss 2.467990, test_acc 0.180000\n",
      "Epoch 471, train_loss 2.007331 train_acc 0.590000, test_loss 2.467795, test_acc 0.180000\n",
      "Epoch 472, train_loss 2.024482 train_acc 0.565000, test_loss 2.467837, test_acc 0.180000\n",
      "Epoch 473, train_loss 2.057977 train_acc 0.515000, test_loss 2.468013, test_acc 0.180000\n",
      "Epoch 474, train_loss 2.039512 train_acc 0.580000, test_loss 2.468234, test_acc 0.180000\n",
      "Epoch 475, train_loss 2.068001 train_acc 0.540000, test_loss 2.468362, test_acc 0.180000\n",
      "Epoch 476, train_loss 2.042913 train_acc 0.540000, test_loss 2.468423, test_acc 0.180000\n",
      "Epoch 477, train_loss 2.037721 train_acc 0.575000, test_loss 2.468474, test_acc 0.180000\n",
      "Epoch 478, train_loss 2.065066 train_acc 0.525000, test_loss 2.468452, test_acc 0.180000\n",
      "Epoch 479, train_loss 2.054462 train_acc 0.580000, test_loss 2.468466, test_acc 0.180000\n",
      "Epoch 480, train_loss 2.017073 train_acc 0.590000, test_loss 2.468393, test_acc 0.180000\n",
      "Epoch 481, train_loss 2.065921 train_acc 0.530000, test_loss 2.468102, test_acc 0.180000\n",
      "Epoch 482, train_loss 2.036411 train_acc 0.585000, test_loss 2.468139, test_acc 0.180000\n",
      "Epoch 483, train_loss 2.033729 train_acc 0.560000, test_loss 2.468256, test_acc 0.180000\n",
      "Epoch 484, train_loss 2.041426 train_acc 0.545000, test_loss 2.468388, test_acc 0.180000\n",
      "Epoch 485, train_loss 2.037240 train_acc 0.565000, test_loss 2.468509, test_acc 0.180000\n",
      "Epoch 486, train_loss 2.084960 train_acc 0.515000, test_loss 2.468604, test_acc 0.180000\n",
      "Epoch 487, train_loss 2.042102 train_acc 0.520000, test_loss 2.468423, test_acc 0.180000\n",
      "Epoch 488, train_loss 2.034962 train_acc 0.555000, test_loss 2.468156, test_acc 0.180000\n",
      "Epoch 489, train_loss 2.064157 train_acc 0.535000, test_loss 2.467983, test_acc 0.180000\n",
      "Epoch 490, train_loss 2.078937 train_acc 0.505000, test_loss 2.467839, test_acc 0.180000\n",
      "Epoch 491, train_loss 1.981258 train_acc 0.635000, test_loss 2.467761, test_acc 0.180000\n",
      "Epoch 492, train_loss 2.073722 train_acc 0.515000, test_loss 2.467787, test_acc 0.180000\n",
      "Epoch 493, train_loss 1.984340 train_acc 0.625000, test_loss 2.467976, test_acc 0.180000\n",
      "Epoch 494, train_loss 2.007878 train_acc 0.585000, test_loss 2.468221, test_acc 0.180000\n",
      "Epoch 495, train_loss 2.034955 train_acc 0.555000, test_loss 2.468543, test_acc 0.180000\n",
      "Epoch 496, train_loss 1.996459 train_acc 0.580000, test_loss 2.468762, test_acc 0.160000\n",
      "Epoch 497, train_loss 2.078401 train_acc 0.540000, test_loss 2.468935, test_acc 0.160000\n",
      "Epoch 498, train_loss 2.048851 train_acc 0.545000, test_loss 2.469000, test_acc 0.160000\n",
      "Epoch 499, train_loss 2.072834 train_acc 0.530000, test_loss 2.469122, test_acc 0.160000\n",
      "Epoch 500, train_loss 1.984863 train_acc 0.640000, test_loss 2.469087, test_acc 0.160000\n",
      "Epoch 501, train_loss 2.049635 train_acc 0.550000, test_loss 2.468990, test_acc 0.160000\n",
      "Epoch 502, train_loss 2.062851 train_acc 0.520000, test_loss 2.468957, test_acc 0.160000\n",
      "Epoch 503, train_loss 2.075745 train_acc 0.510000, test_loss 2.468788, test_acc 0.180000\n",
      "Epoch 504, train_loss 2.024579 train_acc 0.585000, test_loss 2.468678, test_acc 0.180000\n",
      "Epoch 505, train_loss 2.022308 train_acc 0.560000, test_loss 2.468568, test_acc 0.180000\n",
      "Epoch 506, train_loss 2.010625 train_acc 0.600000, test_loss 2.468403, test_acc 0.180000\n",
      "Epoch 507, train_loss 2.028334 train_acc 0.565000, test_loss 2.468244, test_acc 0.180000\n",
      "Epoch 508, train_loss 2.079229 train_acc 0.500000, test_loss 2.468245, test_acc 0.180000\n",
      "Epoch 509, train_loss 2.019168 train_acc 0.560000, test_loss 2.468087, test_acc 0.180000\n",
      "Epoch 510, train_loss 2.013953 train_acc 0.600000, test_loss 2.468044, test_acc 0.180000\n",
      "Epoch 511, train_loss 2.112912 train_acc 0.490000, test_loss 2.468134, test_acc 0.180000\n",
      "Epoch 512, train_loss 1.999236 train_acc 0.600000, test_loss 2.468188, test_acc 0.180000\n",
      "Epoch 513, train_loss 2.041299 train_acc 0.540000, test_loss 2.468285, test_acc 0.180000\n",
      "Epoch 514, train_loss 2.045206 train_acc 0.540000, test_loss 2.468391, test_acc 0.180000\n",
      "Epoch 515, train_loss 2.030657 train_acc 0.565000, test_loss 2.468421, test_acc 0.160000\n",
      "Epoch 516, train_loss 2.039557 train_acc 0.585000, test_loss 2.468434, test_acc 0.160000\n",
      "Epoch 517, train_loss 2.083919 train_acc 0.525000, test_loss 2.468517, test_acc 0.160000\n",
      "Epoch 518, train_loss 2.036249 train_acc 0.550000, test_loss 2.468427, test_acc 0.180000\n",
      "Epoch 519, train_loss 2.052362 train_acc 0.545000, test_loss 2.468431, test_acc 0.180000\n",
      "Epoch 520, train_loss 2.040332 train_acc 0.545000, test_loss 2.468436, test_acc 0.180000\n",
      "Epoch 521, train_loss 2.052865 train_acc 0.565000, test_loss 2.468252, test_acc 0.180000\n",
      "Epoch 522, train_loss 2.059387 train_acc 0.555000, test_loss 2.467992, test_acc 0.180000\n",
      "Epoch 523, train_loss 2.085810 train_acc 0.510000, test_loss 2.467696, test_acc 0.180000\n",
      "Epoch 524, train_loss 2.075548 train_acc 0.540000, test_loss 2.467516, test_acc 0.180000\n",
      "Epoch 525, train_loss 2.043580 train_acc 0.550000, test_loss 2.467264, test_acc 0.180000\n",
      "Epoch 526, train_loss 2.014093 train_acc 0.595000, test_loss 2.466937, test_acc 0.180000\n",
      "Epoch 527, train_loss 2.026686 train_acc 0.570000, test_loss 2.466763, test_acc 0.180000\n",
      "Epoch 528, train_loss 2.028428 train_acc 0.580000, test_loss 2.466787, test_acc 0.180000\n",
      "Epoch 529, train_loss 2.070859 train_acc 0.520000, test_loss 2.466865, test_acc 0.180000\n",
      "Epoch 530, train_loss 2.039459 train_acc 0.570000, test_loss 2.467029, test_acc 0.180000\n",
      "Epoch 531, train_loss 2.020853 train_acc 0.570000, test_loss 2.467174, test_acc 0.180000\n",
      "Epoch 532, train_loss 2.019836 train_acc 0.550000, test_loss 2.467406, test_acc 0.180000\n",
      "Epoch 533, train_loss 2.033486 train_acc 0.545000, test_loss 2.467740, test_acc 0.180000\n",
      "Epoch 534, train_loss 2.067729 train_acc 0.525000, test_loss 2.468019, test_acc 0.180000\n",
      "Epoch 535, train_loss 2.067147 train_acc 0.540000, test_loss 2.468088, test_acc 0.180000\n",
      "Epoch 536, train_loss 2.051591 train_acc 0.560000, test_loss 2.468275, test_acc 0.180000\n",
      "Epoch 537, train_loss 2.093799 train_acc 0.540000, test_loss 2.468514, test_acc 0.180000\n",
      "Epoch 538, train_loss 2.023632 train_acc 0.580000, test_loss 2.468737, test_acc 0.180000\n",
      "Epoch 539, train_loss 2.001468 train_acc 0.575000, test_loss 2.468812, test_acc 0.180000\n",
      "Epoch 540, train_loss 2.071839 train_acc 0.520000, test_loss 2.468809, test_acc 0.160000\n",
      "Epoch 541, train_loss 2.035530 train_acc 0.575000, test_loss 2.468683, test_acc 0.160000\n",
      "Epoch 542, train_loss 2.040735 train_acc 0.555000, test_loss 2.468517, test_acc 0.180000\n",
      "Epoch 543, train_loss 2.053360 train_acc 0.550000, test_loss 2.468213, test_acc 0.180000\n",
      "Epoch 544, train_loss 2.081634 train_acc 0.515000, test_loss 2.468002, test_acc 0.180000\n",
      "Epoch 545, train_loss 2.026263 train_acc 0.570000, test_loss 2.467991, test_acc 0.180000\n",
      "Epoch 546, train_loss 2.055935 train_acc 0.570000, test_loss 2.468048, test_acc 0.180000\n",
      "Epoch 547, train_loss 2.059663 train_acc 0.565000, test_loss 2.468156, test_acc 0.180000\n",
      "Epoch 548, train_loss 2.077742 train_acc 0.515000, test_loss 2.468313, test_acc 0.180000\n",
      "Epoch 549, train_loss 2.040822 train_acc 0.550000, test_loss 2.468373, test_acc 0.180000\n",
      "Epoch 550, train_loss 2.041045 train_acc 0.550000, test_loss 2.468272, test_acc 0.180000\n",
      "Epoch 551, train_loss 2.038443 train_acc 0.555000, test_loss 2.468061, test_acc 0.180000\n",
      "Epoch 552, train_loss 2.051535 train_acc 0.540000, test_loss 2.467626, test_acc 0.180000\n",
      "Epoch 553, train_loss 2.019407 train_acc 0.590000, test_loss 2.467428, test_acc 0.180000\n",
      "Epoch 554, train_loss 2.049483 train_acc 0.595000, test_loss 2.467353, test_acc 0.180000\n",
      "Epoch 555, train_loss 2.089871 train_acc 0.480000, test_loss 2.467317, test_acc 0.180000\n",
      "Epoch 556, train_loss 2.063242 train_acc 0.535000, test_loss 2.467359, test_acc 0.180000\n",
      "Epoch 557, train_loss 2.033952 train_acc 0.585000, test_loss 2.467615, test_acc 0.180000\n",
      "Epoch 558, train_loss 2.023744 train_acc 0.580000, test_loss 2.467739, test_acc 0.180000\n",
      "Epoch 559, train_loss 2.023633 train_acc 0.575000, test_loss 2.467857, test_acc 0.180000\n",
      "Epoch 560, train_loss 2.071557 train_acc 0.510000, test_loss 2.467789, test_acc 0.180000\n",
      "Epoch 561, train_loss 2.033368 train_acc 0.545000, test_loss 2.467775, test_acc 0.180000\n",
      "Epoch 562, train_loss 2.064554 train_acc 0.515000, test_loss 2.467836, test_acc 0.180000\n",
      "Epoch 563, train_loss 2.029126 train_acc 0.555000, test_loss 2.467811, test_acc 0.180000\n",
      "Epoch 564, train_loss 2.078035 train_acc 0.545000, test_loss 2.467809, test_acc 0.180000\n",
      "Epoch 565, train_loss 2.022022 train_acc 0.585000, test_loss 2.468005, test_acc 0.180000\n",
      "Epoch 566, train_loss 2.049702 train_acc 0.550000, test_loss 2.468172, test_acc 0.180000\n",
      "Epoch 567, train_loss 2.035650 train_acc 0.560000, test_loss 2.468166, test_acc 0.180000\n",
      "Epoch 568, train_loss 2.006026 train_acc 0.590000, test_loss 2.468023, test_acc 0.180000\n",
      "Epoch 569, train_loss 2.049418 train_acc 0.525000, test_loss 2.467885, test_acc 0.180000\n",
      "Epoch 570, train_loss 2.050183 train_acc 0.530000, test_loss 2.467811, test_acc 0.180000\n",
      "Epoch 571, train_loss 2.006816 train_acc 0.585000, test_loss 2.467811, test_acc 0.180000\n",
      "Epoch 572, train_loss 2.073305 train_acc 0.525000, test_loss 2.467764, test_acc 0.180000\n",
      "Epoch 573, train_loss 2.055898 train_acc 0.555000, test_loss 2.467607, test_acc 0.180000\n",
      "Epoch 574, train_loss 2.044939 train_acc 0.555000, test_loss 2.467386, test_acc 0.180000\n",
      "Epoch 575, train_loss 2.037510 train_acc 0.565000, test_loss 2.467099, test_acc 0.180000\n",
      "Epoch 576, train_loss 1.982161 train_acc 0.630000, test_loss 2.466782, test_acc 0.180000\n",
      "Epoch 577, train_loss 2.027656 train_acc 0.550000, test_loss 2.466641, test_acc 0.180000\n",
      "Epoch 578, train_loss 2.062946 train_acc 0.540000, test_loss 2.466528, test_acc 0.180000\n",
      "Epoch 579, train_loss 2.060365 train_acc 0.535000, test_loss 2.466485, test_acc 0.180000\n",
      "Epoch 580, train_loss 1.996834 train_acc 0.625000, test_loss 2.466613, test_acc 0.180000\n",
      "Epoch 581, train_loss 2.036038 train_acc 0.565000, test_loss 2.466758, test_acc 0.180000\n",
      "Epoch 582, train_loss 2.043092 train_acc 0.555000, test_loss 2.466954, test_acc 0.180000\n",
      "Epoch 583, train_loss 2.027002 train_acc 0.575000, test_loss 2.467086, test_acc 0.180000\n",
      "Epoch 584, train_loss 2.104257 train_acc 0.495000, test_loss 2.467297, test_acc 0.180000\n",
      "Epoch 585, train_loss 1.988979 train_acc 0.620000, test_loss 2.467488, test_acc 0.180000\n",
      "Epoch 586, train_loss 1.986284 train_acc 0.605000, test_loss 2.467468, test_acc 0.180000\n",
      "Epoch 587, train_loss 1.998722 train_acc 0.635000, test_loss 2.467468, test_acc 0.180000\n",
      "Epoch 588, train_loss 2.048246 train_acc 0.555000, test_loss 2.467433, test_acc 0.180000\n",
      "Epoch 589, train_loss 2.028985 train_acc 0.575000, test_loss 2.467350, test_acc 0.180000\n",
      "Epoch 590, train_loss 2.027007 train_acc 0.575000, test_loss 2.467174, test_acc 0.180000\n",
      "Epoch 591, train_loss 2.060371 train_acc 0.540000, test_loss 2.467002, test_acc 0.180000\n",
      "Epoch 592, train_loss 2.037292 train_acc 0.575000, test_loss 2.466738, test_acc 0.180000\n",
      "Epoch 593, train_loss 2.030448 train_acc 0.550000, test_loss 2.466508, test_acc 0.180000\n",
      "Epoch 594, train_loss 2.065081 train_acc 0.515000, test_loss 2.466263, test_acc 0.180000\n",
      "Epoch 595, train_loss 2.017267 train_acc 0.575000, test_loss 2.466125, test_acc 0.180000\n",
      "Epoch 596, train_loss 2.032545 train_acc 0.580000, test_loss 2.466093, test_acc 0.180000\n",
      "Epoch 597, train_loss 2.098322 train_acc 0.485000, test_loss 2.466252, test_acc 0.180000\n",
      "Epoch 598, train_loss 2.055290 train_acc 0.545000, test_loss 2.466322, test_acc 0.180000\n",
      "Epoch 599, train_loss 2.057637 train_acc 0.540000, test_loss 2.466527, test_acc 0.180000\n",
      "Epoch 600, train_loss 2.056850 train_acc 0.540000, test_loss 2.466813, test_acc 0.180000\n",
      "Epoch 601, train_loss 2.047901 train_acc 0.565000, test_loss 2.467066, test_acc 0.180000\n",
      "Epoch 602, train_loss 2.043346 train_acc 0.545000, test_loss 2.467353, test_acc 0.180000\n",
      "Epoch 603, train_loss 2.029662 train_acc 0.570000, test_loss 2.467410, test_acc 0.180000\n",
      "Epoch 604, train_loss 2.050189 train_acc 0.555000, test_loss 2.467441, test_acc 0.180000\n",
      "Epoch 605, train_loss 2.020888 train_acc 0.580000, test_loss 2.467512, test_acc 0.180000\n",
      "Epoch 606, train_loss 2.060159 train_acc 0.535000, test_loss 2.467570, test_acc 0.180000\n",
      "Epoch 607, train_loss 2.010355 train_acc 0.595000, test_loss 2.467613, test_acc 0.180000\n",
      "Epoch 608, train_loss 2.007572 train_acc 0.570000, test_loss 2.467584, test_acc 0.180000\n",
      "Epoch 609, train_loss 2.046700 train_acc 0.535000, test_loss 2.467449, test_acc 0.180000\n",
      "Epoch 610, train_loss 2.094166 train_acc 0.515000, test_loss 2.467343, test_acc 0.180000\n",
      "Epoch 611, train_loss 2.018231 train_acc 0.575000, test_loss 2.467368, test_acc 0.180000\n",
      "Epoch 612, train_loss 2.031493 train_acc 0.580000, test_loss 2.467439, test_acc 0.180000\n",
      "Epoch 613, train_loss 2.024654 train_acc 0.565000, test_loss 2.467859, test_acc 0.180000\n",
      "Epoch 614, train_loss 2.068671 train_acc 0.510000, test_loss 2.468017, test_acc 0.160000\n",
      "Epoch 615, train_loss 2.029711 train_acc 0.545000, test_loss 2.468141, test_acc 0.160000\n",
      "Epoch 616, train_loss 2.056219 train_acc 0.525000, test_loss 2.468190, test_acc 0.160000\n",
      "Epoch 617, train_loss 2.002893 train_acc 0.585000, test_loss 2.468206, test_acc 0.160000\n",
      "Epoch 618, train_loss 2.012102 train_acc 0.575000, test_loss 2.468129, test_acc 0.160000\n",
      "Epoch 619, train_loss 2.039965 train_acc 0.565000, test_loss 2.468049, test_acc 0.180000\n",
      "Epoch 620, train_loss 2.078153 train_acc 0.510000, test_loss 2.467762, test_acc 0.180000\n",
      "Epoch 621, train_loss 2.084001 train_acc 0.490000, test_loss 2.467270, test_acc 0.180000\n",
      "Epoch 622, train_loss 2.101118 train_acc 0.500000, test_loss 2.466818, test_acc 0.180000\n",
      "Epoch 623, train_loss 2.034685 train_acc 0.555000, test_loss 2.466650, test_acc 0.180000\n",
      "Epoch 624, train_loss 2.045143 train_acc 0.545000, test_loss 2.466536, test_acc 0.180000\n",
      "Epoch 625, train_loss 2.043428 train_acc 0.545000, test_loss 2.466686, test_acc 0.180000\n",
      "Epoch 626, train_loss 2.002975 train_acc 0.585000, test_loss 2.466728, test_acc 0.180000\n",
      "Epoch 627, train_loss 2.020641 train_acc 0.580000, test_loss 2.466648, test_acc 0.180000\n",
      "Epoch 628, train_loss 2.074208 train_acc 0.530000, test_loss 2.466618, test_acc 0.180000\n",
      "Epoch 629, train_loss 2.059851 train_acc 0.545000, test_loss 2.466562, test_acc 0.180000\n",
      "Epoch 630, train_loss 2.061483 train_acc 0.540000, test_loss 2.466555, test_acc 0.180000\n",
      "Epoch 631, train_loss 2.069588 train_acc 0.540000, test_loss 2.466600, test_acc 0.180000\n",
      "Epoch 632, train_loss 2.044468 train_acc 0.535000, test_loss 2.466751, test_acc 0.180000\n",
      "Epoch 633, train_loss 2.060565 train_acc 0.530000, test_loss 2.466747, test_acc 0.180000\n",
      "Epoch 634, train_loss 2.067679 train_acc 0.520000, test_loss 2.466911, test_acc 0.180000\n",
      "Epoch 635, train_loss 2.073928 train_acc 0.490000, test_loss 2.467204, test_acc 0.180000\n",
      "Epoch 636, train_loss 2.050728 train_acc 0.570000, test_loss 2.467584, test_acc 0.180000\n",
      "Epoch 637, train_loss 2.084802 train_acc 0.525000, test_loss 2.467887, test_acc 0.180000\n",
      "Epoch 638, train_loss 2.046518 train_acc 0.550000, test_loss 2.468073, test_acc 0.180000\n",
      "Epoch 639, train_loss 2.064103 train_acc 0.515000, test_loss 2.468250, test_acc 0.180000\n",
      "Epoch 640, train_loss 1.991190 train_acc 0.620000, test_loss 2.468420, test_acc 0.180000\n",
      "Epoch 641, train_loss 2.027793 train_acc 0.560000, test_loss 2.468590, test_acc 0.180000\n",
      "Epoch 642, train_loss 2.042478 train_acc 0.560000, test_loss 2.468640, test_acc 0.180000\n",
      "Epoch 643, train_loss 2.005928 train_acc 0.580000, test_loss 2.468490, test_acc 0.180000\n",
      "Epoch 644, train_loss 2.005044 train_acc 0.585000, test_loss 2.468332, test_acc 0.180000\n",
      "Epoch 645, train_loss 2.102515 train_acc 0.470000, test_loss 2.468342, test_acc 0.180000\n",
      "Epoch 646, train_loss 2.068767 train_acc 0.535000, test_loss 2.468418, test_acc 0.180000\n",
      "Epoch 647, train_loss 2.031780 train_acc 0.590000, test_loss 2.468386, test_acc 0.180000\n",
      "Epoch 648, train_loss 2.050667 train_acc 0.525000, test_loss 2.468179, test_acc 0.180000\n",
      "Epoch 649, train_loss 2.043721 train_acc 0.550000, test_loss 2.468051, test_acc 0.180000\n",
      "Epoch 650, train_loss 2.058532 train_acc 0.535000, test_loss 2.468020, test_acc 0.180000\n",
      "Epoch 651, train_loss 2.020081 train_acc 0.580000, test_loss 2.468075, test_acc 0.180000\n",
      "Epoch 652, train_loss 2.068111 train_acc 0.510000, test_loss 2.468077, test_acc 0.180000\n",
      "Epoch 653, train_loss 2.010324 train_acc 0.600000, test_loss 2.467995, test_acc 0.180000\n",
      "Epoch 654, train_loss 2.044325 train_acc 0.550000, test_loss 2.467870, test_acc 0.180000\n",
      "Epoch 655, train_loss 1.969611 train_acc 0.625000, test_loss 2.467763, test_acc 0.180000\n",
      "Epoch 656, train_loss 2.016651 train_acc 0.560000, test_loss 2.467750, test_acc 0.180000\n",
      "Epoch 657, train_loss 2.034187 train_acc 0.550000, test_loss 2.467768, test_acc 0.180000\n",
      "Epoch 658, train_loss 2.010879 train_acc 0.570000, test_loss 2.467887, test_acc 0.180000\n",
      "Epoch 659, train_loss 2.082941 train_acc 0.530000, test_loss 2.468029, test_acc 0.180000\n",
      "Epoch 660, train_loss 2.045798 train_acc 0.550000, test_loss 2.468111, test_acc 0.180000\n",
      "Epoch 661, train_loss 2.040218 train_acc 0.565000, test_loss 2.468050, test_acc 0.180000\n",
      "Epoch 662, train_loss 2.037442 train_acc 0.555000, test_loss 2.468014, test_acc 0.180000\n",
      "Epoch 663, train_loss 2.030894 train_acc 0.570000, test_loss 2.467955, test_acc 0.180000\n",
      "Epoch 664, train_loss 2.029589 train_acc 0.570000, test_loss 2.467792, test_acc 0.180000\n",
      "Epoch 665, train_loss 2.081422 train_acc 0.525000, test_loss 2.467582, test_acc 0.180000\n",
      "Epoch 666, train_loss 2.075199 train_acc 0.520000, test_loss 2.467425, test_acc 0.180000\n",
      "Epoch 667, train_loss 2.056888 train_acc 0.555000, test_loss 2.467367, test_acc 0.180000\n",
      "Epoch 668, train_loss 2.028695 train_acc 0.565000, test_loss 2.467219, test_acc 0.180000\n",
      "Epoch 669, train_loss 2.012341 train_acc 0.585000, test_loss 2.467078, test_acc 0.180000\n",
      "Epoch 670, train_loss 2.049496 train_acc 0.540000, test_loss 2.467006, test_acc 0.180000\n",
      "Epoch 671, train_loss 2.042100 train_acc 0.540000, test_loss 2.466909, test_acc 0.180000\n",
      "Epoch 672, train_loss 2.088137 train_acc 0.535000, test_loss 2.466862, test_acc 0.180000\n",
      "Epoch 673, train_loss 2.048706 train_acc 0.565000, test_loss 2.466849, test_acc 0.180000\n",
      "Epoch 674, train_loss 1.965790 train_acc 0.615000, test_loss 2.466835, test_acc 0.180000\n",
      "Epoch 675, train_loss 2.037052 train_acc 0.565000, test_loss 2.466834, test_acc 0.180000\n",
      "Epoch 676, train_loss 2.033573 train_acc 0.585000, test_loss 2.466926, test_acc 0.180000\n",
      "Epoch 677, train_loss 2.072321 train_acc 0.555000, test_loss 2.466879, test_acc 0.180000\n",
      "Epoch 678, train_loss 2.067772 train_acc 0.525000, test_loss 2.466917, test_acc 0.180000\n",
      "Epoch 679, train_loss 1.983930 train_acc 0.610000, test_loss 2.466870, test_acc 0.180000\n",
      "Epoch 680, train_loss 1.991310 train_acc 0.630000, test_loss 2.466784, test_acc 0.180000\n",
      "Epoch 681, train_loss 2.034819 train_acc 0.565000, test_loss 2.466764, test_acc 0.180000\n",
      "Epoch 682, train_loss 2.061731 train_acc 0.540000, test_loss 2.466813, test_acc 0.180000\n",
      "Epoch 683, train_loss 2.061987 train_acc 0.520000, test_loss 2.466890, test_acc 0.180000\n",
      "Epoch 684, train_loss 2.036052 train_acc 0.560000, test_loss 2.467031, test_acc 0.180000\n",
      "Epoch 685, train_loss 2.049048 train_acc 0.555000, test_loss 2.467096, test_acc 0.180000\n",
      "Epoch 686, train_loss 2.046579 train_acc 0.525000, test_loss 2.467113, test_acc 0.180000\n",
      "Epoch 687, train_loss 2.102648 train_acc 0.505000, test_loss 2.467142, test_acc 0.180000\n",
      "Epoch 688, train_loss 2.073327 train_acc 0.520000, test_loss 2.467305, test_acc 0.180000\n",
      "Epoch 689, train_loss 2.129887 train_acc 0.475000, test_loss 2.467488, test_acc 0.180000\n",
      "Epoch 690, train_loss 2.044163 train_acc 0.530000, test_loss 2.467534, test_acc 0.180000\n",
      "Epoch 691, train_loss 2.062257 train_acc 0.500000, test_loss 2.467618, test_acc 0.180000\n",
      "Epoch 692, train_loss 2.050101 train_acc 0.525000, test_loss 2.467725, test_acc 0.180000\n",
      "Epoch 693, train_loss 2.016617 train_acc 0.575000, test_loss 2.467775, test_acc 0.180000\n",
      "Epoch 694, train_loss 2.075570 train_acc 0.520000, test_loss 2.467909, test_acc 0.180000\n",
      "Epoch 695, train_loss 2.023078 train_acc 0.600000, test_loss 2.467969, test_acc 0.180000\n",
      "Epoch 696, train_loss 2.083636 train_acc 0.530000, test_loss 2.467938, test_acc 0.180000\n",
      "Epoch 697, train_loss 2.100275 train_acc 0.500000, test_loss 2.467763, test_acc 0.180000\n",
      "Epoch 698, train_loss 2.052177 train_acc 0.545000, test_loss 2.467545, test_acc 0.180000\n",
      "Epoch 699, train_loss 2.031619 train_acc 0.545000, test_loss 2.467252, test_acc 0.180000\n",
      "Epoch 700, train_loss 2.031357 train_acc 0.570000, test_loss 2.467042, test_acc 0.180000\n",
      "Epoch 701, train_loss 2.061784 train_acc 0.535000, test_loss 2.467107, test_acc 0.180000\n",
      "Epoch 702, train_loss 2.091056 train_acc 0.525000, test_loss 2.467125, test_acc 0.180000\n",
      "Epoch 703, train_loss 2.036405 train_acc 0.540000, test_loss 2.467252, test_acc 0.180000\n",
      "Epoch 704, train_loss 2.050500 train_acc 0.545000, test_loss 2.467422, test_acc 0.180000\n",
      "Epoch 705, train_loss 2.030662 train_acc 0.585000, test_loss 2.467647, test_acc 0.180000\n",
      "Epoch 706, train_loss 2.071318 train_acc 0.500000, test_loss 2.467881, test_acc 0.180000\n",
      "Epoch 707, train_loss 2.038590 train_acc 0.570000, test_loss 2.467973, test_acc 0.180000\n",
      "Epoch 708, train_loss 2.048525 train_acc 0.545000, test_loss 2.467962, test_acc 0.180000\n",
      "Epoch 709, train_loss 2.027577 train_acc 0.575000, test_loss 2.468009, test_acc 0.180000\n",
      "Epoch 710, train_loss 2.081137 train_acc 0.540000, test_loss 2.468017, test_acc 0.180000\n",
      "Epoch 711, train_loss 2.020901 train_acc 0.595000, test_loss 2.468073, test_acc 0.180000\n",
      "Epoch 712, train_loss 2.042957 train_acc 0.555000, test_loss 2.468214, test_acc 0.180000\n",
      "Epoch 713, train_loss 2.004115 train_acc 0.615000, test_loss 2.468430, test_acc 0.180000\n",
      "Epoch 714, train_loss 2.033117 train_acc 0.560000, test_loss 2.468681, test_acc 0.180000\n",
      "Epoch 715, train_loss 2.048043 train_acc 0.570000, test_loss 2.468899, test_acc 0.180000\n",
      "Epoch 716, train_loss 2.012620 train_acc 0.610000, test_loss 2.469053, test_acc 0.180000\n",
      "Epoch 717, train_loss 2.077269 train_acc 0.525000, test_loss 2.469169, test_acc 0.180000\n",
      "Epoch 718, train_loss 2.028932 train_acc 0.585000, test_loss 2.469267, test_acc 0.180000\n",
      "Epoch 719, train_loss 2.045667 train_acc 0.550000, test_loss 2.469313, test_acc 0.180000\n",
      "Epoch 720, train_loss 2.035164 train_acc 0.545000, test_loss 2.469202, test_acc 0.180000\n",
      "Epoch 721, train_loss 2.016482 train_acc 0.600000, test_loss 2.469041, test_acc 0.180000\n",
      "Epoch 722, train_loss 2.051463 train_acc 0.530000, test_loss 2.468827, test_acc 0.180000\n",
      "Epoch 723, train_loss 2.034281 train_acc 0.560000, test_loss 2.468471, test_acc 0.180000\n",
      "Epoch 724, train_loss 2.025038 train_acc 0.590000, test_loss 2.468010, test_acc 0.180000\n",
      "Epoch 725, train_loss 2.044652 train_acc 0.570000, test_loss 2.467704, test_acc 0.180000\n",
      "Epoch 726, train_loss 2.074681 train_acc 0.500000, test_loss 2.467442, test_acc 0.180000\n",
      "Epoch 727, train_loss 2.072783 train_acc 0.540000, test_loss 2.467208, test_acc 0.180000\n",
      "Epoch 728, train_loss 2.006392 train_acc 0.590000, test_loss 2.467070, test_acc 0.180000\n",
      "Epoch 729, train_loss 2.031047 train_acc 0.580000, test_loss 2.466850, test_acc 0.180000\n",
      "Epoch 730, train_loss 2.007384 train_acc 0.600000, test_loss 2.466533, test_acc 0.180000\n",
      "Epoch 731, train_loss 2.081300 train_acc 0.510000, test_loss 2.466223, test_acc 0.180000\n",
      "Epoch 732, train_loss 2.005215 train_acc 0.610000, test_loss 2.465993, test_acc 0.180000\n",
      "Epoch 733, train_loss 2.037392 train_acc 0.545000, test_loss 2.465900, test_acc 0.180000\n",
      "Epoch 734, train_loss 2.060158 train_acc 0.520000, test_loss 2.465922, test_acc 0.180000\n",
      "Epoch 735, train_loss 2.020755 train_acc 0.580000, test_loss 2.465844, test_acc 0.180000\n",
      "Epoch 736, train_loss 2.015511 train_acc 0.585000, test_loss 2.465875, test_acc 0.180000\n",
      "Epoch 737, train_loss 2.012286 train_acc 0.605000, test_loss 2.465984, test_acc 0.180000\n",
      "Epoch 738, train_loss 1.996290 train_acc 0.600000, test_loss 2.466128, test_acc 0.180000\n",
      "Epoch 739, train_loss 2.049238 train_acc 0.510000, test_loss 2.466236, test_acc 0.180000\n",
      "Epoch 740, train_loss 2.089482 train_acc 0.500000, test_loss 2.466312, test_acc 0.180000\n",
      "Epoch 741, train_loss 1.985976 train_acc 0.620000, test_loss 2.466367, test_acc 0.180000\n",
      "Epoch 742, train_loss 2.043257 train_acc 0.560000, test_loss 2.466331, test_acc 0.180000\n",
      "Epoch 743, train_loss 2.043597 train_acc 0.550000, test_loss 2.466374, test_acc 0.180000\n",
      "Epoch 744, train_loss 2.014780 train_acc 0.590000, test_loss 2.466569, test_acc 0.180000\n",
      "Epoch 745, train_loss 2.057206 train_acc 0.545000, test_loss 2.466789, test_acc 0.180000\n",
      "Epoch 746, train_loss 2.026675 train_acc 0.580000, test_loss 2.466930, test_acc 0.180000\n",
      "Epoch 747, train_loss 2.065199 train_acc 0.530000, test_loss 2.466918, test_acc 0.180000\n",
      "Epoch 748, train_loss 1.994465 train_acc 0.620000, test_loss 2.467001, test_acc 0.180000\n",
      "Epoch 749, train_loss 2.015195 train_acc 0.570000, test_loss 2.467173, test_acc 0.180000\n",
      "Epoch 750, train_loss 2.017910 train_acc 0.595000, test_loss 2.467419, test_acc 0.180000\n",
      "Epoch 751, train_loss 2.068086 train_acc 0.510000, test_loss 2.467686, test_acc 0.180000\n",
      "Epoch 752, train_loss 2.056446 train_acc 0.545000, test_loss 2.467838, test_acc 0.180000\n",
      "Epoch 753, train_loss 2.057512 train_acc 0.540000, test_loss 2.468016, test_acc 0.180000\n",
      "Epoch 754, train_loss 2.037588 train_acc 0.555000, test_loss 2.468084, test_acc 0.180000\n",
      "Epoch 755, train_loss 2.037962 train_acc 0.550000, test_loss 2.467998, test_acc 0.180000\n",
      "Epoch 756, train_loss 1.970761 train_acc 0.620000, test_loss 2.468046, test_acc 0.180000\n",
      "Epoch 757, train_loss 2.024523 train_acc 0.605000, test_loss 2.468157, test_acc 0.180000\n",
      "Epoch 758, train_loss 2.032764 train_acc 0.585000, test_loss 2.468286, test_acc 0.180000\n",
      "Epoch 759, train_loss 1.988946 train_acc 0.610000, test_loss 2.468348, test_acc 0.180000\n",
      "Epoch 760, train_loss 2.050069 train_acc 0.530000, test_loss 2.468257, test_acc 0.180000\n",
      "Epoch 761, train_loss 2.025958 train_acc 0.555000, test_loss 2.468041, test_acc 0.180000\n",
      "Epoch 762, train_loss 2.037056 train_acc 0.550000, test_loss 2.467845, test_acc 0.180000\n",
      "Epoch 763, train_loss 2.096143 train_acc 0.480000, test_loss 2.467775, test_acc 0.180000\n",
      "Epoch 764, train_loss 2.033240 train_acc 0.545000, test_loss 2.467695, test_acc 0.180000\n",
      "Epoch 765, train_loss 2.097282 train_acc 0.525000, test_loss 2.467663, test_acc 0.180000\n",
      "Epoch 766, train_loss 2.031988 train_acc 0.565000, test_loss 2.467628, test_acc 0.180000\n",
      "Epoch 767, train_loss 2.011337 train_acc 0.575000, test_loss 2.467539, test_acc 0.180000\n",
      "Epoch 768, train_loss 2.056731 train_acc 0.550000, test_loss 2.467388, test_acc 0.180000\n",
      "Epoch 769, train_loss 2.070944 train_acc 0.535000, test_loss 2.467234, test_acc 0.180000\n",
      "Epoch 770, train_loss 2.045039 train_acc 0.550000, test_loss 2.467146, test_acc 0.180000\n",
      "Epoch 771, train_loss 2.054184 train_acc 0.545000, test_loss 2.467139, test_acc 0.180000\n",
      "Epoch 772, train_loss 2.091768 train_acc 0.520000, test_loss 2.467183, test_acc 0.180000\n",
      "Epoch 773, train_loss 2.023570 train_acc 0.565000, test_loss 2.467205, test_acc 0.180000\n",
      "Epoch 774, train_loss 2.025068 train_acc 0.555000, test_loss 2.467231, test_acc 0.180000\n",
      "Epoch 775, train_loss 2.056769 train_acc 0.535000, test_loss 2.467268, test_acc 0.180000\n",
      "Epoch 776, train_loss 2.044264 train_acc 0.545000, test_loss 2.467376, test_acc 0.180000\n",
      "Epoch 777, train_loss 2.026241 train_acc 0.570000, test_loss 2.467446, test_acc 0.180000\n",
      "Epoch 778, train_loss 2.068680 train_acc 0.520000, test_loss 2.467428, test_acc 0.180000\n",
      "Epoch 779, train_loss 2.028266 train_acc 0.545000, test_loss 2.467436, test_acc 0.180000\n",
      "Epoch 780, train_loss 2.071009 train_acc 0.520000, test_loss 2.467669, test_acc 0.180000\n",
      "Epoch 781, train_loss 2.078196 train_acc 0.520000, test_loss 2.467800, test_acc 0.180000\n",
      "Epoch 782, train_loss 2.060564 train_acc 0.545000, test_loss 2.467882, test_acc 0.180000\n",
      "Epoch 783, train_loss 2.015354 train_acc 0.580000, test_loss 2.467916, test_acc 0.180000\n",
      "Epoch 784, train_loss 2.051618 train_acc 0.570000, test_loss 2.467941, test_acc 0.180000\n",
      "Epoch 785, train_loss 1.997918 train_acc 0.595000, test_loss 2.468005, test_acc 0.180000\n",
      "Epoch 786, train_loss 2.024811 train_acc 0.575000, test_loss 2.468005, test_acc 0.180000\n",
      "Epoch 787, train_loss 2.027667 train_acc 0.570000, test_loss 2.468010, test_acc 0.180000\n",
      "Epoch 788, train_loss 2.058790 train_acc 0.540000, test_loss 2.467932, test_acc 0.180000\n",
      "Epoch 789, train_loss 2.049998 train_acc 0.550000, test_loss 2.467825, test_acc 0.180000\n",
      "Epoch 790, train_loss 2.041184 train_acc 0.560000, test_loss 2.467759, test_acc 0.180000\n",
      "Epoch 791, train_loss 2.039895 train_acc 0.555000, test_loss 2.467913, test_acc 0.180000\n",
      "Epoch 792, train_loss 2.079260 train_acc 0.505000, test_loss 2.468088, test_acc 0.180000\n",
      "Epoch 793, train_loss 2.070796 train_acc 0.515000, test_loss 2.468222, test_acc 0.180000\n",
      "Epoch 794, train_loss 2.033958 train_acc 0.540000, test_loss 2.468248, test_acc 0.180000\n",
      "Epoch 795, train_loss 2.056257 train_acc 0.555000, test_loss 2.468023, test_acc 0.180000\n",
      "Epoch 796, train_loss 2.030549 train_acc 0.580000, test_loss 2.467742, test_acc 0.180000\n",
      "Epoch 797, train_loss 2.050001 train_acc 0.525000, test_loss 2.467613, test_acc 0.180000\n",
      "Epoch 798, train_loss 2.049965 train_acc 0.550000, test_loss 2.467474, test_acc 0.180000\n",
      "Epoch 799, train_loss 2.045531 train_acc 0.525000, test_loss 2.467193, test_acc 0.180000\n",
      "Epoch 800, train_loss 2.094506 train_acc 0.505000, test_loss 2.466822, test_acc 0.180000\n",
      "Epoch 801, train_loss 2.029959 train_acc 0.555000, test_loss 2.466496, test_acc 0.180000\n",
      "Epoch 802, train_loss 2.061666 train_acc 0.565000, test_loss 2.466189, test_acc 0.180000\n",
      "Epoch 803, train_loss 2.042632 train_acc 0.565000, test_loss 2.466028, test_acc 0.180000\n",
      "Epoch 804, train_loss 1.983926 train_acc 0.620000, test_loss 2.465791, test_acc 0.180000\n",
      "Epoch 805, train_loss 2.013497 train_acc 0.610000, test_loss 2.465431, test_acc 0.180000\n",
      "Epoch 806, train_loss 2.041961 train_acc 0.535000, test_loss 2.465172, test_acc 0.180000\n",
      "Epoch 807, train_loss 2.016762 train_acc 0.570000, test_loss 2.465110, test_acc 0.180000\n",
      "Epoch 808, train_loss 2.000638 train_acc 0.580000, test_loss 2.465148, test_acc 0.180000\n",
      "Epoch 809, train_loss 2.085953 train_acc 0.505000, test_loss 2.465293, test_acc 0.180000\n",
      "Epoch 810, train_loss 2.048714 train_acc 0.560000, test_loss 2.465526, test_acc 0.180000\n",
      "Epoch 811, train_loss 2.024852 train_acc 0.575000, test_loss 2.465822, test_acc 0.180000\n",
      "Epoch 812, train_loss 2.053738 train_acc 0.535000, test_loss 2.466136, test_acc 0.180000\n",
      "Epoch 813, train_loss 2.056952 train_acc 0.535000, test_loss 2.466533, test_acc 0.180000\n",
      "Epoch 814, train_loss 2.038181 train_acc 0.575000, test_loss 2.466897, test_acc 0.180000\n",
      "Epoch 815, train_loss 2.038237 train_acc 0.545000, test_loss 2.467201, test_acc 0.180000\n",
      "Epoch 816, train_loss 1.969376 train_acc 0.645000, test_loss 2.467508, test_acc 0.180000\n",
      "Epoch 817, train_loss 2.095945 train_acc 0.460000, test_loss 2.467695, test_acc 0.180000\n",
      "Epoch 818, train_loss 2.053161 train_acc 0.555000, test_loss 2.467785, test_acc 0.180000\n",
      "Epoch 819, train_loss 2.067362 train_acc 0.535000, test_loss 2.467871, test_acc 0.180000\n",
      "Epoch 820, train_loss 2.082360 train_acc 0.515000, test_loss 2.467895, test_acc 0.180000\n",
      "Epoch 821, train_loss 2.049603 train_acc 0.570000, test_loss 2.467877, test_acc 0.180000\n",
      "Epoch 822, train_loss 2.042152 train_acc 0.575000, test_loss 2.467859, test_acc 0.180000\n",
      "Epoch 823, train_loss 2.020011 train_acc 0.580000, test_loss 2.467836, test_acc 0.180000\n",
      "Epoch 824, train_loss 2.030471 train_acc 0.585000, test_loss 2.467824, test_acc 0.180000\n",
      "Epoch 825, train_loss 2.031895 train_acc 0.580000, test_loss 2.467623, test_acc 0.180000\n",
      "Epoch 826, train_loss 2.033420 train_acc 0.560000, test_loss 2.467432, test_acc 0.180000\n",
      "Epoch 827, train_loss 2.034878 train_acc 0.560000, test_loss 2.467227, test_acc 0.180000\n",
      "Epoch 828, train_loss 2.047100 train_acc 0.550000, test_loss 2.467012, test_acc 0.180000\n",
      "Epoch 829, train_loss 2.025317 train_acc 0.555000, test_loss 2.466814, test_acc 0.180000\n",
      "Epoch 830, train_loss 2.015869 train_acc 0.590000, test_loss 2.466707, test_acc 0.180000\n",
      "Epoch 831, train_loss 2.028840 train_acc 0.590000, test_loss 2.466637, test_acc 0.180000\n",
      "Epoch 832, train_loss 2.054413 train_acc 0.550000, test_loss 2.466725, test_acc 0.180000\n",
      "Epoch 833, train_loss 2.044335 train_acc 0.525000, test_loss 2.466900, test_acc 0.180000\n",
      "Epoch 834, train_loss 2.058857 train_acc 0.545000, test_loss 2.467112, test_acc 0.180000\n",
      "Epoch 835, train_loss 2.071302 train_acc 0.490000, test_loss 2.467223, test_acc 0.180000\n",
      "Epoch 836, train_loss 2.053616 train_acc 0.540000, test_loss 2.467350, test_acc 0.180000\n",
      "Epoch 837, train_loss 2.016590 train_acc 0.575000, test_loss 2.467325, test_acc 0.180000\n",
      "Epoch 838, train_loss 2.024870 train_acc 0.575000, test_loss 2.467358, test_acc 0.180000\n",
      "Epoch 839, train_loss 2.034402 train_acc 0.540000, test_loss 2.467333, test_acc 0.180000\n",
      "Epoch 840, train_loss 2.099295 train_acc 0.485000, test_loss 2.467277, test_acc 0.180000\n",
      "Epoch 841, train_loss 2.105329 train_acc 0.530000, test_loss 2.467232, test_acc 0.180000\n",
      "Epoch 842, train_loss 2.062410 train_acc 0.545000, test_loss 2.467363, test_acc 0.180000\n",
      "Epoch 843, train_loss 2.050549 train_acc 0.555000, test_loss 2.467472, test_acc 0.180000\n",
      "Epoch 844, train_loss 2.020733 train_acc 0.575000, test_loss 2.467513, test_acc 0.180000\n",
      "Epoch 845, train_loss 2.024097 train_acc 0.565000, test_loss 2.467602, test_acc 0.180000\n",
      "Epoch 846, train_loss 2.004004 train_acc 0.585000, test_loss 2.467686, test_acc 0.180000\n",
      "Epoch 847, train_loss 2.056492 train_acc 0.530000, test_loss 2.467638, test_acc 0.180000\n",
      "Epoch 848, train_loss 1.991624 train_acc 0.615000, test_loss 2.467643, test_acc 0.180000\n",
      "Epoch 849, train_loss 2.072294 train_acc 0.515000, test_loss 2.467583, test_acc 0.180000\n",
      "Epoch 850, train_loss 2.024807 train_acc 0.550000, test_loss 2.467624, test_acc 0.180000\n",
      "Epoch 851, train_loss 2.056777 train_acc 0.540000, test_loss 2.467581, test_acc 0.180000\n",
      "Epoch 852, train_loss 1.991744 train_acc 0.605000, test_loss 2.467496, test_acc 0.180000\n",
      "Epoch 853, train_loss 2.090640 train_acc 0.540000, test_loss 2.467473, test_acc 0.180000\n",
      "Epoch 854, train_loss 1.975842 train_acc 0.625000, test_loss 2.467531, test_acc 0.180000\n",
      "Epoch 855, train_loss 2.040707 train_acc 0.550000, test_loss 2.467599, test_acc 0.180000\n",
      "Epoch 856, train_loss 2.018567 train_acc 0.575000, test_loss 2.467720, test_acc 0.180000\n",
      "Epoch 857, train_loss 2.057720 train_acc 0.535000, test_loss 2.467627, test_acc 0.180000\n",
      "Epoch 858, train_loss 2.062881 train_acc 0.540000, test_loss 2.467453, test_acc 0.180000\n",
      "Epoch 859, train_loss 2.022114 train_acc 0.575000, test_loss 2.467615, test_acc 0.180000\n",
      "Epoch 860, train_loss 2.104739 train_acc 0.500000, test_loss 2.467673, test_acc 0.180000\n",
      "Epoch 861, train_loss 2.069797 train_acc 0.505000, test_loss 2.467736, test_acc 0.180000\n",
      "Epoch 862, train_loss 2.046608 train_acc 0.535000, test_loss 2.467931, test_acc 0.180000\n",
      "Epoch 863, train_loss 2.013714 train_acc 0.565000, test_loss 2.468117, test_acc 0.180000\n",
      "Epoch 864, train_loss 2.039181 train_acc 0.525000, test_loss 2.468275, test_acc 0.180000\n",
      "Epoch 865, train_loss 2.034835 train_acc 0.565000, test_loss 2.468437, test_acc 0.180000\n",
      "Epoch 866, train_loss 2.033211 train_acc 0.545000, test_loss 2.468585, test_acc 0.180000\n",
      "Epoch 867, train_loss 2.028417 train_acc 0.585000, test_loss 2.468508, test_acc 0.180000\n",
      "Epoch 868, train_loss 2.059674 train_acc 0.535000, test_loss 2.468373, test_acc 0.180000\n",
      "Epoch 869, train_loss 2.047882 train_acc 0.565000, test_loss 2.468152, test_acc 0.180000\n",
      "Epoch 870, train_loss 2.059640 train_acc 0.555000, test_loss 2.467916, test_acc 0.180000\n",
      "Epoch 871, train_loss 2.058029 train_acc 0.555000, test_loss 2.467812, test_acc 0.180000\n",
      "Epoch 872, train_loss 2.041387 train_acc 0.560000, test_loss 2.467679, test_acc 0.180000\n",
      "Epoch 873, train_loss 2.035812 train_acc 0.565000, test_loss 2.467505, test_acc 0.180000\n",
      "Epoch 874, train_loss 2.028685 train_acc 0.545000, test_loss 2.467120, test_acc 0.180000\n",
      "Epoch 875, train_loss 1.972979 train_acc 0.610000, test_loss 2.466785, test_acc 0.180000\n",
      "Epoch 876, train_loss 2.033342 train_acc 0.575000, test_loss 2.466512, test_acc 0.180000\n",
      "Epoch 877, train_loss 2.055757 train_acc 0.545000, test_loss 2.466363, test_acc 0.180000\n",
      "Epoch 878, train_loss 1.989898 train_acc 0.585000, test_loss 2.466175, test_acc 0.180000\n",
      "Epoch 879, train_loss 2.040274 train_acc 0.550000, test_loss 2.466023, test_acc 0.180000\n",
      "Epoch 880, train_loss 2.062995 train_acc 0.530000, test_loss 2.465981, test_acc 0.180000\n",
      "Epoch 881, train_loss 2.016835 train_acc 0.595000, test_loss 2.466033, test_acc 0.180000\n",
      "Epoch 882, train_loss 2.074921 train_acc 0.525000, test_loss 2.466040, test_acc 0.180000\n",
      "Epoch 883, train_loss 2.050950 train_acc 0.540000, test_loss 2.466012, test_acc 0.180000\n",
      "Epoch 884, train_loss 2.038724 train_acc 0.540000, test_loss 2.466138, test_acc 0.180000\n",
      "Epoch 885, train_loss 2.050324 train_acc 0.570000, test_loss 2.466296, test_acc 0.180000\n",
      "Epoch 886, train_loss 2.027804 train_acc 0.590000, test_loss 2.466397, test_acc 0.180000\n",
      "Epoch 887, train_loss 2.058156 train_acc 0.565000, test_loss 2.466602, test_acc 0.180000\n",
      "Epoch 888, train_loss 2.089383 train_acc 0.500000, test_loss 2.466819, test_acc 0.180000\n",
      "Epoch 889, train_loss 2.031759 train_acc 0.565000, test_loss 2.466982, test_acc 0.180000\n",
      "Epoch 890, train_loss 2.055975 train_acc 0.550000, test_loss 2.467072, test_acc 0.180000\n",
      "Epoch 891, train_loss 2.052152 train_acc 0.555000, test_loss 2.467259, test_acc 0.180000\n",
      "Epoch 892, train_loss 2.016796 train_acc 0.575000, test_loss 2.467363, test_acc 0.180000\n",
      "Epoch 893, train_loss 2.053912 train_acc 0.525000, test_loss 2.467393, test_acc 0.180000\n",
      "Epoch 894, train_loss 2.021929 train_acc 0.555000, test_loss 2.467355, test_acc 0.180000\n",
      "Epoch 895, train_loss 2.047600 train_acc 0.525000, test_loss 2.467269, test_acc 0.180000\n",
      "Epoch 896, train_loss 2.073553 train_acc 0.525000, test_loss 2.467251, test_acc 0.180000\n",
      "Epoch 897, train_loss 2.009847 train_acc 0.575000, test_loss 2.467297, test_acc 0.180000\n",
      "Epoch 898, train_loss 2.022737 train_acc 0.575000, test_loss 2.467333, test_acc 0.180000\n",
      "Epoch 899, train_loss 2.013474 train_acc 0.575000, test_loss 2.467336, test_acc 0.180000\n",
      "Epoch 900, train_loss 1.993189 train_acc 0.580000, test_loss 2.467145, test_acc 0.180000\n",
      "Epoch 901, train_loss 2.007823 train_acc 0.590000, test_loss 2.466933, test_acc 0.180000\n",
      "Epoch 902, train_loss 2.029862 train_acc 0.560000, test_loss 2.466782, test_acc 0.180000\n",
      "Epoch 903, train_loss 2.011991 train_acc 0.580000, test_loss 2.466644, test_acc 0.180000\n",
      "Epoch 904, train_loss 2.075099 train_acc 0.545000, test_loss 2.466587, test_acc 0.180000\n",
      "Epoch 905, train_loss 2.053539 train_acc 0.530000, test_loss 2.466591, test_acc 0.180000\n",
      "Epoch 906, train_loss 2.048306 train_acc 0.525000, test_loss 2.466580, test_acc 0.180000\n",
      "Epoch 907, train_loss 2.083921 train_acc 0.495000, test_loss 2.466507, test_acc 0.180000\n",
      "Epoch 908, train_loss 2.023872 train_acc 0.580000, test_loss 2.466483, test_acc 0.180000\n",
      "Epoch 909, train_loss 2.088538 train_acc 0.510000, test_loss 2.466448, test_acc 0.180000\n",
      "Epoch 910, train_loss 2.039558 train_acc 0.565000, test_loss 2.466403, test_acc 0.180000\n",
      "Epoch 911, train_loss 2.062521 train_acc 0.545000, test_loss 2.466502, test_acc 0.180000\n",
      "Epoch 912, train_loss 2.040982 train_acc 0.570000, test_loss 2.466697, test_acc 0.180000\n",
      "Epoch 913, train_loss 2.041132 train_acc 0.565000, test_loss 2.466878, test_acc 0.180000\n",
      "Epoch 914, train_loss 2.008504 train_acc 0.585000, test_loss 2.466955, test_acc 0.180000\n",
      "Epoch 915, train_loss 2.053738 train_acc 0.520000, test_loss 2.467006, test_acc 0.180000\n",
      "Epoch 916, train_loss 2.029063 train_acc 0.580000, test_loss 2.466878, test_acc 0.180000\n",
      "Epoch 917, train_loss 2.081184 train_acc 0.530000, test_loss 2.466824, test_acc 0.180000\n",
      "Epoch 918, train_loss 1.997198 train_acc 0.600000, test_loss 2.466900, test_acc 0.180000\n",
      "Epoch 919, train_loss 2.038694 train_acc 0.565000, test_loss 2.467037, test_acc 0.180000\n",
      "Epoch 920, train_loss 2.044701 train_acc 0.555000, test_loss 2.467187, test_acc 0.180000\n",
      "Epoch 921, train_loss 2.032124 train_acc 0.525000, test_loss 2.467339, test_acc 0.180000\n",
      "Epoch 922, train_loss 2.043133 train_acc 0.570000, test_loss 2.467360, test_acc 0.180000\n",
      "Epoch 923, train_loss 2.057506 train_acc 0.525000, test_loss 2.467307, test_acc 0.180000\n",
      "Epoch 924, train_loss 2.062199 train_acc 0.520000, test_loss 2.467102, test_acc 0.180000\n",
      "Epoch 925, train_loss 2.056303 train_acc 0.515000, test_loss 2.466918, test_acc 0.180000\n",
      "Epoch 926, train_loss 2.061098 train_acc 0.515000, test_loss 2.466828, test_acc 0.180000\n",
      "Epoch 927, train_loss 2.030854 train_acc 0.565000, test_loss 2.466800, test_acc 0.180000\n",
      "Epoch 928, train_loss 2.015557 train_acc 0.595000, test_loss 2.466909, test_acc 0.180000\n",
      "Epoch 929, train_loss 2.080235 train_acc 0.515000, test_loss 2.467028, test_acc 0.180000\n",
      "Epoch 930, train_loss 2.055808 train_acc 0.525000, test_loss 2.467055, test_acc 0.180000\n",
      "Epoch 931, train_loss 2.023544 train_acc 0.565000, test_loss 2.467001, test_acc 0.180000\n",
      "Epoch 932, train_loss 2.034552 train_acc 0.560000, test_loss 2.467007, test_acc 0.180000\n",
      "Epoch 933, train_loss 1.994830 train_acc 0.620000, test_loss 2.467167, test_acc 0.180000\n",
      "Epoch 934, train_loss 2.045073 train_acc 0.590000, test_loss 2.467330, test_acc 0.180000\n",
      "Epoch 935, train_loss 2.025982 train_acc 0.560000, test_loss 2.467363, test_acc 0.180000\n",
      "Epoch 936, train_loss 2.051534 train_acc 0.550000, test_loss 2.467370, test_acc 0.180000\n",
      "Epoch 937, train_loss 2.082104 train_acc 0.515000, test_loss 2.467413, test_acc 0.180000\n",
      "Epoch 938, train_loss 2.083535 train_acc 0.505000, test_loss 2.467396, test_acc 0.180000\n",
      "Epoch 939, train_loss 2.088652 train_acc 0.505000, test_loss 2.467379, test_acc 0.180000\n",
      "Epoch 940, train_loss 1.994274 train_acc 0.610000, test_loss 2.467384, test_acc 0.180000\n",
      "Epoch 941, train_loss 2.067080 train_acc 0.540000, test_loss 2.467428, test_acc 0.180000\n",
      "Epoch 942, train_loss 2.038647 train_acc 0.555000, test_loss 2.467559, test_acc 0.180000\n",
      "Epoch 943, train_loss 2.030916 train_acc 0.570000, test_loss 2.467643, test_acc 0.180000\n",
      "Epoch 944, train_loss 2.027818 train_acc 0.570000, test_loss 2.467692, test_acc 0.180000\n",
      "Epoch 945, train_loss 2.044643 train_acc 0.540000, test_loss 2.467607, test_acc 0.180000\n",
      "Epoch 946, train_loss 2.080065 train_acc 0.520000, test_loss 2.467354, test_acc 0.180000\n",
      "Epoch 947, train_loss 2.064476 train_acc 0.545000, test_loss 2.467128, test_acc 0.180000\n",
      "Epoch 948, train_loss 2.093026 train_acc 0.510000, test_loss 2.466904, test_acc 0.180000\n",
      "Epoch 949, train_loss 2.049723 train_acc 0.550000, test_loss 2.466721, test_acc 0.180000\n",
      "Epoch 950, train_loss 2.053442 train_acc 0.540000, test_loss 2.466446, test_acc 0.180000\n",
      "Epoch 951, train_loss 2.028366 train_acc 0.555000, test_loss 2.466526, test_acc 0.180000\n",
      "Epoch 952, train_loss 2.087572 train_acc 0.535000, test_loss 2.466804, test_acc 0.180000\n",
      "Epoch 953, train_loss 2.078776 train_acc 0.510000, test_loss 2.466796, test_acc 0.180000\n",
      "Epoch 954, train_loss 2.000129 train_acc 0.595000, test_loss 2.466478, test_acc 0.180000\n",
      "Epoch 955, train_loss 2.023300 train_acc 0.565000, test_loss 2.466303, test_acc 0.180000\n",
      "Epoch 956, train_loss 2.048155 train_acc 0.550000, test_loss 2.466284, test_acc 0.180000\n",
      "Epoch 957, train_loss 2.038937 train_acc 0.565000, test_loss 2.466354, test_acc 0.180000\n",
      "Epoch 958, train_loss 2.053258 train_acc 0.535000, test_loss 2.466496, test_acc 0.180000\n",
      "Epoch 959, train_loss 2.069368 train_acc 0.540000, test_loss 2.466689, test_acc 0.180000\n",
      "Epoch 960, train_loss 2.023545 train_acc 0.560000, test_loss 2.466893, test_acc 0.180000\n",
      "Epoch 961, train_loss 2.074430 train_acc 0.505000, test_loss 2.467082, test_acc 0.180000\n",
      "Epoch 962, train_loss 2.061055 train_acc 0.560000, test_loss 2.467181, test_acc 0.180000\n",
      "Epoch 963, train_loss 2.041651 train_acc 0.535000, test_loss 2.467106, test_acc 0.180000\n",
      "Epoch 964, train_loss 2.048224 train_acc 0.535000, test_loss 2.467130, test_acc 0.180000\n",
      "Epoch 965, train_loss 2.043980 train_acc 0.550000, test_loss 2.467127, test_acc 0.180000\n",
      "Epoch 966, train_loss 2.065792 train_acc 0.545000, test_loss 2.467122, test_acc 0.180000\n",
      "Epoch 967, train_loss 1.984328 train_acc 0.630000, test_loss 2.467117, test_acc 0.180000\n",
      "Epoch 968, train_loss 2.030507 train_acc 0.545000, test_loss 2.467143, test_acc 0.180000\n",
      "Epoch 969, train_loss 2.058151 train_acc 0.535000, test_loss 2.467134, test_acc 0.180000\n",
      "Epoch 970, train_loss 2.086285 train_acc 0.490000, test_loss 2.467104, test_acc 0.180000\n",
      "Epoch 971, train_loss 2.030143 train_acc 0.560000, test_loss 2.466964, test_acc 0.180000\n",
      "Epoch 972, train_loss 2.035284 train_acc 0.565000, test_loss 2.466789, test_acc 0.180000\n",
      "Epoch 973, train_loss 2.061659 train_acc 0.530000, test_loss 2.466659, test_acc 0.180000\n",
      "Epoch 974, train_loss 2.063468 train_acc 0.545000, test_loss 2.466532, test_acc 0.180000\n",
      "Epoch 975, train_loss 2.013362 train_acc 0.575000, test_loss 2.466535, test_acc 0.180000\n",
      "Epoch 976, train_loss 2.047449 train_acc 0.565000, test_loss 2.466578, test_acc 0.180000\n",
      "Epoch 977, train_loss 2.032592 train_acc 0.565000, test_loss 2.466602, test_acc 0.180000\n",
      "Epoch 978, train_loss 2.061098 train_acc 0.540000, test_loss 2.466627, test_acc 0.180000\n",
      "Epoch 979, train_loss 2.025480 train_acc 0.575000, test_loss 2.466792, test_acc 0.180000\n",
      "Epoch 980, train_loss 2.092407 train_acc 0.480000, test_loss 2.467051, test_acc 0.180000\n",
      "Epoch 981, train_loss 2.044878 train_acc 0.550000, test_loss 2.467239, test_acc 0.180000\n",
      "Epoch 982, train_loss 2.013443 train_acc 0.570000, test_loss 2.467308, test_acc 0.180000\n",
      "Epoch 983, train_loss 2.061729 train_acc 0.535000, test_loss 2.467401, test_acc 0.180000\n",
      "Epoch 984, train_loss 2.044120 train_acc 0.540000, test_loss 2.467419, test_acc 0.180000\n",
      "Epoch 985, train_loss 2.077155 train_acc 0.505000, test_loss 2.467453, test_acc 0.180000\n",
      "Epoch 986, train_loss 2.032931 train_acc 0.575000, test_loss 2.467482, test_acc 0.180000\n",
      "Epoch 987, train_loss 2.059841 train_acc 0.525000, test_loss 2.467495, test_acc 0.180000\n",
      "Epoch 988, train_loss 1.985448 train_acc 0.600000, test_loss 2.467458, test_acc 0.180000\n",
      "Epoch 989, train_loss 2.012818 train_acc 0.590000, test_loss 2.467421, test_acc 0.180000\n",
      "Epoch 990, train_loss 2.027705 train_acc 0.565000, test_loss 2.467387, test_acc 0.180000\n",
      "Epoch 991, train_loss 2.026897 train_acc 0.555000, test_loss 2.467412, test_acc 0.180000\n",
      "Epoch 992, train_loss 2.038875 train_acc 0.575000, test_loss 2.467390, test_acc 0.180000\n",
      "Epoch 993, train_loss 2.007798 train_acc 0.580000, test_loss 2.467256, test_acc 0.180000\n",
      "Epoch 994, train_loss 2.007370 train_acc 0.605000, test_loss 2.467166, test_acc 0.180000\n",
      "Epoch 995, train_loss 2.033557 train_acc 0.550000, test_loss 2.467022, test_acc 0.180000\n",
      "Epoch 996, train_loss 2.099238 train_acc 0.485000, test_loss 2.466884, test_acc 0.180000\n",
      "Epoch 997, train_loss 2.073324 train_acc 0.515000, test_loss 2.466812, test_acc 0.180000\n",
      "Epoch 998, train_loss 2.034383 train_acc 0.570000, test_loss 2.466886, test_acc 0.180000\n",
      "Epoch 999, train_loss 2.020438 train_acc 0.565000, test_loss 2.466940, test_acc 0.180000\n",
      "Epoch 1000, train_loss 2.053469 train_acc 0.540000, test_loss 2.466934, test_acc 0.180000\n"
     ]
    }
   ],
   "source": [
    "train_loss_, train_acc_, test_loss_, test_acc_ = [], [], [], []\n",
    "no_epochs = 1000\n",
    "\n",
    "# initialize early stopper\n",
    "# early_stopper = EarlyStopper(patience=3, min_delta=0)\n",
    "\n",
    "# start training\n",
    "for epoch in range(no_epochs):\n",
    "    train_loss, train_acc = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loss, test_acc = test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "    train_loss_.append(train_loss), train_acc_.append(train_acc)\n",
    "    test_loss_.append(test_loss), test_acc_.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, train_loss {train_loss:>7f} train_acc {train_acc:>4f}, test_loss {test_loss:>7f}, test_acc {test_acc:>4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
